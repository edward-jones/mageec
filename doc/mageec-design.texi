\input texinfo
@setfilename mageec-design.info
@afourpaper
@settitle MAGEEC Design
@paragraphindent 0

@copying
This design document is for MAGEEC.
This manual is for MAGEEC.

Copyright @copyright{} 2015 Embecosm Limited.
@quotation
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, with no Front-Cover Texts, and with no Back-Cover
Texts. A copy of the license is included in the section entitled
``GNU Free Documentation License''.
@end quotation
@end copying

@titlepage
@title MAGEEC Design
@subtitle MAchine Guided Energy Efficient Compilation
@author Edward Jones
@page
@vskip 0pt plus 1filll
@insertcopying
@end titlepage

@contents

@node Top
@top Scope of this Document
The document outlines the design of MAGEEC @emph{MAchine Guided Energy
Efficient Compilation}.

@menu
* MAGEEC Design::
* GNU Free Documentation License::
@end menu

@node MAGEEC Design
@chapter MAGEEC Design

@section Overview

MAGEEC is a set of components to augment a compiler with the ability
to make intelligent predictions about which optimizations to run based
on a machine learner trained against statically extracted features of
the program under compilation.

Contemporary compilers mostly use a fixed pipeline of passes when
optimizing. This pipeline can often be configured by the user, either
at a high level to specify the overall degree of optimization desired,
or at a low level to enable or disable specific optimizations, however
for a given invocation of the compiler the pipeline is fixed. The
pass pipelines for common optimization levels are balanced to work
well with the widest range of code which may pass through the
compiler, however for any given piece of code it will not necessarily
be optimal. In many compilers the user may tune the pipeline for a
given program or source file, however this is a non-trivial manual
task which requires a lot of trial and error, and a lot of
iterations of compilation.

@c image of typical compile approach
@image{"current-pass-pipelines"} @c filename[, width[, height[, alttext[, extension]]]]}

From the perspective of the user, it would be reassuring to know
that the compiler is not missing optimization opportunities. If the
compiler could be relied on to make more accurate optimization decisions
it could also be feasible to simplify the user experience by
not requiring them to manually disable and enable flags to
squeeze maximum performance from their code.

From the perspective of the compiler writer, it would be beneificial
to avoid the laborious task of tuning the pass pipeline for each new
processor and CPU. For a given architecture, there can exist a vast
array of CPUs which vary in the hardware features supported and
the intricacies of their microarchitecture. @c ARM example?
For each microarchitecture it is necessary to optimize the
compiler to target it, and automatically optimizing the pass
pipeline is a way in which performance can be specialized for a
given core requiring minimal input from the compiler writer.

A further point of interest is that most compilers currently
optimize for speed, and give less attention to code size, energy,
or any other interesting metrics. In general optimization can only
be done with one goal (typically speed or code size) and it is not
possible to express more complex optimizations goals such as
optimizing for energy and code size at the same time, or
optimizing for speed whilst keeping compile time to a minimum.

If selecting the best compiler configuration can be automated,
then there are several potential benefits which can be realized.
  
@itemize
@item
Remove complexity from the compiler to tune optimization
pipelines for different ISAs, microarchitectures, CPU features
sets, and metrics.

@item
Provide the user a more straightforward interface, as well
as better performing programs.

@item
Control optimizations at a finer granularity (file, module) to
realize performance improvements which cannot be realized in current
compilers (which can only control the pass pipeline at a
per-invocation level).

@item
Optimize for new metrics such as energy or compilation speed.

@item
Provide a more powerful interface to control tradeoffs
between different metrics, such as balancing energy and size.
@end itemize

@c Diagram of more ideal optimization approach
@image{"better-pass-pipelines"} @c filename[, width[, height[, alttext[, extension]]]]}

@section Other optimization approaches

As it stands, most compilers appear to use a fixed pipeline of
passes when optimizing. However there has been other work to make
pass control more powerful. Some of these techniques exist in
contemporary compilers, however many only appear in research.

@itemize
@item
Profile-guided optimization: This is common in many open source
and propreitary compilers now. Optimization is tuned based on
profiling data from running the program with representative
data. The profile provides information about the hot-spots in
the code, which can be used to inform the ordering of basic
blocks and the ferocity of inlining.

Profile guided optimization can produce considerable
performance improvements (around 15%). However there are a
couple of drawbacks. First, it requires a project to be
built twice which can be cumbersome for larger software
projects. Second, it requires a long run of the profiled
program with representative data to produce an accurate
profile. Finally, current PGO implementations only provide a
limited amount of information about the program; namely the
frequently of usage of functions and basic blocks. Nevertheless
it is a very valueable technique.

@item
MILEPOST: Much like the aim of MAGEEC, MILEPOST augmented
a compiler with the ability to make optimization decisions
based on statically derived features of an input program.

@c TODO Fill in more info here

@item
The MAGEEC project

@c MAGEEC project
@c citations for other authors work

@end itemize

@section Terminology

@itemize
@item
Program Unit - A contiguous part of a program which can be uniquely
identifier in both the original source code and the final executable.
Examples are modules, functions and possibly loops and basic blocks.

@item
Feature - A measurable and quantifiable aspect of a program unit, which
represents an aspect of the program unit in some way. For example,
the number of instructions in a basic block.

@item
Parameter - A value which can be controlled by some mechanism in the
compiler to affect the decree or nature of optimization, with the
effect of changing the object code produced by the program. For example:
a flag indicating whether the 'dead code elimination' pass should be
executed.

@item
Compiler configuration - A set of parameters used to compile an
individual program unit. Multiple units of the same program may
use the same configuration of the compiler.
@end itemize

@section Theory for how MAGEEC works

MAGEEC automates the selection of optimizations by using a
machine learner to may key decisions about the pass to
run and any tunable parameters which may be set. An
association is learned between features derived from the
input source code, and the appropriate parameters to use in
the compilation to build the executable. Like all machine
learning processes, there is a training stage; where the
machine learner is fed features and corresponding 'good'
parameters to build up an association; and a optimizing stage;
where the machine learner is used with a new, previously
unencountered program and used to select a set of parameters
which are likely to produce a good quality output executable.

@c @image High level diagram of the process (with MAGEEC-instrumented compiler)

@section Comparison with other compiler tuning approaches

MAGEEC uses static features derived ahead of time from the input
program in order to predict parameters which should be used
for a given compilation. However there are a couple of other
approaches which can be used for compiler tuning. One approach
is to predict compiler parameters based on dynamic features
instead of static features, and the other approach is to
iteratively compile a program refining the set of compilation
parameters after each build. Compared to our approaches,
our approach has some advantages and disadvantages:

Advantages:
@itemize
@item
To derive static features, test programs do not have to be recompiled to
receive a benefit. Interactive compilation techniques such as combined
elimination require a large number of recompilation runs, and even just
doing a single profiled run to derived dynamic features can be
prohibitively difficult if the program is large or cannot easily be
instrumented.

@item
Compared to using dynamic features, measuring static features is simpler.
Dynamic features may not be precisely measureable, may depend on the
environment, or require a more complex build stage.

@item
Measuring static features is fast. The effect on compilation time can
be minimized as static features can be very easily derived from the
source code or intermediate representation. This makes deployment
far easier.

@item
In some cases it may not be able to instrument and measure dynamic
features. For example, an embedded system may not have enough memory
for the overhead of instrumentation, or it may not be possible to
measure some features on an embedded systems

@c Example (limitations in debug interface)
@end itemize

Disadvantages:
@itemize
@item
Static features cannot express the effects of the input data on the
program. Two program could have identical features, but exhibit very
different performance characteristics depending on the data provided
to it. Indeed, one program on its own can also exhibit very different
performance depending on the data provided to it. This can be mitigated,
as the features of the program will often hint at the type and patterns
of data usage, however the problem cannot be fully mitigated.

@item
Static features are less representative of the input program. This
relates to the previous point. Dynamic features are likely to be
more representative of the input program than static features. This
means that static features are likely to be far less effective than
using dynamic features. It also means more training data is likely
to be needed.
@end itemize

@section MAGEEC features

The design of MAGEEC makes it possible to use a variety of machine
learners through a common interface, and makes it possible to use
both command-line driven proprietary compilers, and open source
compilers. The interface to MAGEEC can either be a wrapper around
the compiler (ideal for a proprietary compiler), or can integrate
in the compiler itself, possibly through a plugin interface. Deeper
integration is beneficial as it may allow compilation controlling
decision to be made at a finer granularity than program or module
level.

@c Diagram of internal/external MAGEEC control

MAGEEC is open source under the GPL license. The aim is that by doing
this MAGEEC can be freely implemented in as wide range of compilers as
possible.

@section Comparison with original MAGEEC project

The original MAGEEC project aimed at optimizing energy consumption
on embedded systems. The aim of this updated was to additionally
target high performance applications as well as improve the workflow
to make using MAGEEC more straightforward for users.

@section MAGEEC components

@image{"gather-high-level"} @c filename[, width[, height[, alttext[, extension]]]]}
@image{"optimize-high-level"} @c filename[, width[, height[, alttext[, extension]]]]}

MAGEEC itself is made up of a set of components and utilities which
allow a compiler to be instrumented with the ability to predict
good parameters to use in the compilation using machine learners.
The components in MAGEEC include:

@itemize
@item
An interface to derive static features from an input program, as
well as an implementation of a feature extractor for GCC.

@item
A mechanism to drive a compiler to exercise various parameters
during the compilation phase. Currently this is implemented for
GCC as wrapper around the GCC frontend which can alter
optimization flags on a module-by-module basis.

@item
A file format to record features, compilations and results
for programs units in a centralized place. This is implemented
through an SQLite3 database.

@item
An interface which can be implemented by machine learners to
be used with the framework. In addition to this, basic
machine learners are provided in the form of the C5.0 classifier
and a 1NN machine learner.

@item
Tools and scripts to access the file, and drive the training
and optimization process.
@end itemize

@c @image High level diagram of MAGEEC components
@c @image Design of MAGEEC in general

We now describe each of these components in more detail, and
their current implementations.

@subsection Feature extraction

Feature extraction involves taking an input source program, and
outputting a set of features for each of the individual
program units in the program. Each source file can produce
multiple sets of features for each different program unit in 
the file. So the module will produce a feature set, and each
function could produce its own feature set. It is even feasible
that feature could be generated for each basic block or loop.

@subsubsection Interface

MAGEEC provides an interface to create a set of features. How
exactly the features are extracted is up to the program doing the
feature extraction, and the features which are extracted is also
at the whim of the feature extractor. The limitations placed by
MAGEEC are that the type of the features must be one of a set of
known types (this set of types is common with the types which
the machine learners can handle), and a given extracted feature
is consistently identified by the name numerical identifier.

The interface to MAGEEC allows a feature extractor to insert a
new set of features into the file for a program unit, and
receive a unique identifier for those features back.

@subsubsection GCC plugin

@c @image High level diagram of feature extraction process
@c @image Diagram of feature extractor plugin process

As well as the interface to the feature extractor, a feature
extractor plugin in also provided for the GCC compiler. This
plugin can be provided as a plugin to gcc, and used to
extract module and function features from the program.

This plugin takes the input program and outputs a .csv file
identifing an individual program unit in the source code, associating
it with the unique identifier of the feature set for that
program unit.

The benefit of the GCC feature extractor is that it allows feature
extraction to be performed on C, C++ and Fortran code without
requiring a compiler-specific feature extractor to be written.
Provided code can be built with GCC, feature extraction can be
achieved. This makes it ideal for instrumenting proprietary
compilers.

The feature extractor extracts different sets of features
depending on whether a function or a module is the program unit
under compilation. The features extracted for modules are for
the most part just aggregated values based on the values for
each function in that module. The full features for modules
and functions is as follows:

@subsubheading GCC plugin function features

@multitable @columnfractions .25 .75
@headitem Feature name @tab Description
@item Argument count @tab Number of arguments to the function
@c @item Cyclomatic Complexity @tab Cyclomatic complexity of the function
@item CFG edges @tab Number of control flow graph edges
@item Abnormal CFG edges @tab Number of abnormal control flow graph edges
@c @item Critical Path Length @tab Length of the critical path through the function

@c loops
@item Loop count @tab Number of loops in the function
@item Min loop depth @tab Minimum depth of loop in the function
@item Max loop depth @tab Maximum loop depth in the function
@item Loop depth range @tab Difference between the max and minimum loop lengths
@item Mean loop repth @tab Average depth of loops in the function
@item Median loop depth @tab Median depth of loops in the function
@item Loops of depth 1 @tab Count of loops of depth 1
@item Loops of depth 2 @tab Count of loops of depth 2
@item Loops of depth >2 @tab Count of loops of depth >2

@c basic blocks
@item Basic block count @tab Count of basic blocks in the function
@item Basic blocks in loop @tab Number of basic blocks in the function inside a loop
@item Basic blocks outside loop @tab Number of basic blocks in the function outside a loop
@item Max basic block successors @tab Maximum number of successors for any basic block
@item Min basic block successors @tab Minimum number of successors for a basic block
@item Basic block successors range @tab Different between the maximum and minimum number of successors in a basic block
@item Mean basic block successor count @tab Average number of successors for a basic block
@item Median basic block successor count @tab Median number of successors for a basic block
@item Max basic block predecessors @tab Maximum number of predecessors for any basic block
@item Min basic block predecessors @tab Minimum number of predecessors for a basic block
@item Basic block predecessors range @tab Different between the maximum and minimum number of predecessors in a basic block
@item Mean basic block predecessors count @tab Average number of predecessors for a basic block
@item Median basic block predecessors count @tab Median number of predecessors for a basic block
@item Basic blocks with 1 predecessor @tab Count of basic blocks with 1 predecessor
@item Basic blocks with 2 predecessors @tab Count of basic blocks with 2 predecessors
@item Basic blocks with >2 predecessors @tab Count of basic blocks with >2 predecessors
@item Basic blocks with 1 successor @tab Count of basic blocks with 1 successor
@item Basic blocks with 2 successors @tab Count of basic blocks with 2 successors
@item Basic blocks with >2 successors @tab Count of basic blocks with >2 successors
@item Basic blocks with 1 predecessor, 1 successor @tab Count of basic blocks with 1 predecessor and 1 successor
@item Basic blocks with 1 predecessor, 2 successors @tab Count of basic blocks with 1 predecessor and 2 successors
@item Basic blocks with 2 predecessors, 1 successor @tab Count of basic blocks with 2 predecessors and 1 successor
@item Basic blocks with 2 predecessors, 2 successors @tab Count of basic blocks with 2 predecessors and 2 successors
@item Basic blocks with >2 predecessors, >2 successors @tab Count of basic blocks with >2 predecessors and >2 successors

@c instruction counts
@item Instruction count @tab Total number of instructions in the function
@item Max instruction count @tab Max number of instructions in a basic block
@item Mean instruction count @tab Average number of instructions in basic blocks
@item Median instruction count @tab Median instructions in basic blocks

@item Conditional statement count @tab Total number of conditional statements in the function
@item Max conditional statement count @tab Max number of conditional statements in a basic block
@item Mean conditional statement count @tab Average number of conditional statements in a basic block
@item Median conditional statement count @tab Median number of conditional statements in a basic block

@item Direct call count @tab Total number of direct calls in the function
@item Max direct call count @tab Max number of direct calls in a basic block
@item Mean direct call count @tab Average number of direct calls in a basic block
@item Median direct call count @tab Median number of direct calls in a basic block

@item Indirect call count @tab Total number of indirect calls in the function
@item Max indirect call count @tab Max number of indirect calls in a basic block
@item Mean indirect call count @tab Average number of indirect calls in a basic block
@item Median indirect call count @tab Median number of indirect calls in a basic block

@item Integer operations count @tab Total number of integer operations in the function
@item Max integer operations count @tab Max number of integer operations in a basic block
@item Mean integer operations count @tab Average number of integer operations in a basic block
@item Median integer operations count @tab Median number of integer operations in a basic block

@item Floating point operations count @tab Total number of floating point operations in the function
@item Max floating point operations count @tab Max number of floating point operations in a basic block
@item Mean floating point operations count @tab Average number of floating point operations in a basic block
@item Median floating point operations count @tab Median number of floating point operations in a basic block

@item Unary operations count @tab Total number of unary operations in the function
@item Max unary operations count @tab Max number of unary operations in a basic block
@item Mean unary operations count @tab Average number of unary operations in a basic block
@item Median unary operations count @tab Median number of unary operations in a basic block

@item Pointer arithmetic operations count @tab Total number of pointer arithmetic operations in the function
@item Max pointer arithmetic operations count @tab Max number of pointer arithmetic operations in a basic block
@item Mean pointer arithmetic operations count @tab Average number of pointer arithmetic operations in a basic block
@item Median pointer arithmetic operations count @tab Median number of pointer arithmetic operations in a basic block

@item Unconditional branches count @tab Total number of unconditional branches in the function
@item Max unconditional branches count @tab Max number of unconditional branches in a basic block
@item Mean unconditional branches count @tab Average number of unconditional branches in a basic block
@item Median unconditional branches count @tab Median number of unconditional branches in a basic block

@item Assign operations count @tab Total number of assignment operations in the function
@item Max assign operations count @tab Max number of assignment operations in a basic block
@item Mean assign operations count @tab Average number of assignment operations in a basic block
@item Median assign operations count @tab Median number of assignment operations in a basic block

@item Switch statements count @tab Total number of switch statements in the function
@item Max switch statements count @tab Max number of switch statements in a basic block
@item Mean switch statements count @tab Average number of switch statements in a basic block
@item Median switch statements count @tab Median number of switch statements in a basic block

@item Phi nodes count @tab Total number of phi nodes in the function
@item Max phi nodes count @tab Max number of phi nodes in a basic block
@item Mean phi nodes count @tab Average number of phi nodes in a basic block
@item Median phi nodes count @tab Median number of phi nodes in a basic block
@item Phi header nodes count @tab Total number of phi header nodes in the function
@item Max phi header nodes count @tab Max number of phi header nodes in a basic block
@item Mean phi header nodes count @tab Average number of phi header nodes in a basic block
@item Median phi header nodes count @tab Median number of phi header nodes in a basic block
@item Max arguments to phi nodes @tab Max number of arguments to a phi node
@item Mean arguments to phi nodes @tab Average number of arguments to phi nodes
@item Medium arguments to phi nodes @tab Median number of arguments to phi nodes
@item Phi node arguments count between 1 and 5 @tab Number of phi nodes with between 1 and 5 arguments
@item Phi nodes with >5 arguments @tab Number of phi nodes with >5 arguments

@item Max arguments to call instructions @tab Max number of arguments to a call instruction
@item Mean arguments to call instructions @tab Average number of arguments to call instructions
@item Medium arguments to call instructions @tab Medium number of arguments to call instructions
@item Calls with 0 arguments count @tab Number of calls taking no arguments
@item Calls with >=1 and <=3 arguments @tab Number of calls with between 1 and 3 arguments
@item Calls with >3 arguments @tab Number of calls with >3 arguments
@item Max pointer arguments to calls @tab Maximum number of pointer arguments to call instructions
@item Mean pointer arguments to calls @tab Average number of pointer arguments to call instructions
@item Median pointer arguments to calls @tab Median number of pointer arguments to call instructions
@item Number of calls returning integers @tab Number of call instructions returning integers
@item Number of calls returning floating point @tab Number of call instructions returning floating point values
@end multitable

@subsubheading GCC plugin module features

@multitable @columnfractions .25 .75
@headitem Feature name @tab Description
@item Function count @tab Number of functions in the module
@item SCC count @tab Number of strongly connected components in the module
@item Integer returning function count @tab Number of functions returning integers
@item Floating point returning function count @tab Number of functions returning floating-point values

@c loops
@item Min loop depth @tab Min loop depth in functions in the module
@item Max loop depth @tab Max loop depth in functions in the module
@item Mean loop depth @tab Average loop depth in functions in the module
@item Median loop depth @tab Median loop depth in functions in the module
@item Loops of depth 1 @tab Count of loops of depth 1
@item Loops of depth 2 @tab Count of loops of depth 2
@item Loops of depth >2 @tab Count of loops of depth >2

@c functions
@item Min arguments to function @tab Min number of arguments to functions
@item Max arguments to function @tab Max number of arguments to functions
@item Range of arguments to function @tab Difference between the max and min number of arguments to functions
@item Mean arguments to function @tab Average number of arguments to functions
@item Median argument to function @tab Median number of arguments to functions
@c @item Min cylomatic complexity of function @tab Min cyclomatic complexity of functions
@c @item Max cylomatic complexity of function @tab Max cyclomatic complexity of functions
@c @item Range of cylomatic complexity of functions @tab Difference between the max and min cyclomatic complexity of functions
@c @item Mean cylomatic complexity of function @tab Average cyclomatic complexity of functions
@c @item Median cylomatic complexity of function @tab Median cyclomatic complexity of functions
@item Min CFG edges in functions @tab Min number of control flow graph edges of functions
@item Max CFG edges in functions @tab Max number of control flow graph edges of functions
@item Range of CFG edges in functions @tab Difference between the max and min number of control flow graph edges of functions
@item Mean number of CFG edges in functions @tab Average number of control flow graph edges in functions
@item Median number of CFG edges in functions @tab Median number of control flow graph edges in functions
@item Min abnormal CFG edges in functions @tab Min number of abnormal control flow graph edges of functions
@item Max abnormal CFG edges in functions @tab Max number of abnormal control flow graph edges of functions
@item Range of abnormal CFG edges in functions @tab Difference between the max and min number of abnormal control flow graph edges of functions
@item Mean number of abnormal CFG edges in functions @tab Average number of abnormal control flow graph edges in functions
@item Median number of abnormal CFG edges in functions @tab Median number of abnormal control flow graph edges in functions
@c @item Min critical path length of functions @tab Min critical path length of functions
@c @item Max critical path length of functions @tab Max critical path length of functions
@c @item Range of critical path length of functions @tab Different between the max and min critical path length of functions
@c @item Mean critical path length of functions @tab Average critical path length of functions
@c @item Median critical path length of functions @tab Median critical path length of functions

@item Loop count @tab Number of loops in the module
@item Max loop count @tab Max number of loops in a function
@item Min loop count @tab Min number of loops in a function
@item Mean loop count @tab Average number of loops in a function
@item Median loop count @tab Median number of loops in a function

@item Basic block count @tab Number of basic blocks in the module
@item Max basic block count @tab Max number of basic blocks in a function
@item Min basic block count @tab Min number of basic blockss in a function
@item Mean basic block count @tab Average number of basic blocks in a function
@item Median basic block count @tab Median number of basic blocks in a function
@item Basic blocks in loop @tab Number of basic blocks in a loop in the module
@item Max basic blocks in loop @tab Max number of basic blocks in a loop in a function
@item Min basic blocks in loop @tab Min number of basic blocks in a loop in a function
@item Range of basic blocks in loop @tab Difference between the max and min number of basic blocks in a loop in a function
@item Mean basic blocks in loop @tab Average number of basic blocks in a loop in a function
@item Median basic blocks in loop @tab Median number of basic blocks in a loop in a function
@item Basic blocks outside loop @tab Number of basic blocks outside a loop in the module
@item Max basic blocks outside loop @tab Max number of basic blocks outside a loop in a function
@item Min basic blocks outside loop @tab Min number of basic blocks outside a loop in a function
@item Range of basic blocks outside loop @tab Difference between the max and min number of basic blocks outside a loop in a function
@item Mean basic blocks outside loop @tab Average number of basic blocks outside a loop in a function
@item Median basic blocks outside loop @tab Median number of basic blocks outside a loop in a function

@c instructions
@item Instruction count @tab Total number of instructions in the module
@item Max instruction count @tab Max number of instructions in a function
@item Min instruction count @tab Min number of instructions in a function
@item Range of instruction count @tab Difference between max and min number of instructions in a function
@item Mean instruction count @tab Average number of instructions in a function
@item Median instruction count @tab Median instructions in a function

@item Conditional statement count @tab Total number of conditional statements in the module
@item Max conditional statement count @tab Max number of conditional statements in a function
@item Min conditional statement count @tab Min number of conditional statements in a function
@item Range of conditional statement count @tab Difference between max and min number of conditional statements in a function
@item Mean conditional statement count @tab Average number of conditional statements in a function
@item Median conditional statement count @tab Median number of conditional statements in a function

@item Direct call count @tab Total number of direct calls in the module
@item Max direct call count @tab Max number of direct calls in a function
@item Min direct call count @tab Min number of direct calls in a function
@item Range of direct call count @tab Difference between max and min number of direct calls in a function
@item Mean direct call count @tab Average number of direct calls in a function
@item Median direct call count @tab Median number of direct calls in a function

@item Indirect call count @tab Total number of indirect calls in the module
@item Max indirect call count @tab Max number of indirect calls in a function
@item Min indirect call count @tab Min number of indirect calls in a function
@item Range of indirect call count @tab Difference between max and min number of indirect calls in a function
@item Mean indirect call count @tab Average number of indirect calls in a function
@item Median indirect call count @tab Median number of indirect calls in a function

@item Integer operations count @tab Total number of integer operations in the module
@item Max integer operations count @tab Max number of integer operations in a function
@item Mean integer operations count @tab Average number of integer operations in a function
@item Median integer operations count @tab Median number of integer operations in a function

@item Floating point operations count @tab Total number of floating point operations in the module
@item Max floating point operations count @tab Max number of floating point operations in a function
@item Mean floating point operations count @tab Average number of floating point operations in a function
@item Median floating point operations count @tab Median number of floating point operations in a function

@item Unary operations count @tab Total number of unary operations in the module
@item Max unary operations count @tab Max number of unary operations in a function
@item Mean unary operations count @tab Average number of unary operations in a function
@item Median unary operations count @tab Median number of unary operations in a function

@item Pointer arithmetic operations count @tab Total number of pointer arithmetic operations in the module
@item Max pointer arithmetic operations count @tab Max number of pointer arithmetic operations in a function
@item Mean pointer arithmetic operations count @tab Average number of pointer arithmetic operations in a function
@item Median pointer arithmetic operations count @tab Median number of pointer arithmetic operations in a function

@item Unconditional branches count @tab Total number of unconditional branches in the module
@item Max unconditional branches count @tab Max number of unconditional branches in a function
@item Mean unconditional branches count @tab Average number of unconditional branches in a function
@item Median unconditional branches count @tab Median number of unconditional branches in a function

@item Assign operations count @tab Total number of assignment operations in the module
@item Max assign operations count @tab Max number of assignment operations in a function
@item Mean assign operations count @tab Average number of assignment operations in a function
@item Median assign operations count @tab Median number of assignment operations in a function

@item Switch statements count @tab Total number of switch statements in the module
@item Max switch statements count @tab Max number of switch statements in a function
@item Mean switch statements count @tab Average number of switch statements in a function
@item Median switch statements count @tab Median number of switch statements in a function

@item Phi nodes count @tab Total number of phi nodes in the module
@item Max phi nodes count @tab Max number of phi nodes in a function
@item Mean phi nodes count @tab Average number of phi nodes in a function
@item Median phi nodes count @tab Median number of phi nodes in a function
@item Phi header nodes count @tab Total number of phi header nodes in the module
@item Max phi header nodes count @tab Max number of phi header nodes in a function
@item Mean phi header nodes count @tab Average number of phi header nodes in a function
@item Median phi header nodes count @tab Median number of phi header nodes in a function
@end multitable

@subsection Compiler driver

In order to drive the training and optimization process, there needs
to be a mechanism to control the parameters provided to the compiler.
When training, it needs to be possible to run the compiler under a
number of controlled configurations, and record each of these
compilations in the MAGEEC database for later use. During optimization
it needs to be possible to call out to MAGEEC to make decisions using
a trained machine learner and then use these decisions to control
compilation parameters.

@subsubsection Driver interface

MAGEEC offers an interface, which allows the compiler, or a wrapper
controlling the compiler to store a record of each program unit
being compiled, the features associated with that program unit, and
parameters describing the configuration of the compiler for that
compilation. The interface takes all of the information describing
the compilation of a program unit; the features and parameters
involved in the compilation, and returns a unique identifier for
that compilation. Later, results will be associated with the
compilation, and then there will be enough information to train
a machine learner.

For optimization, an interface is also offered which allows the driver
to use a previously trained machine learner to make decisions about
parameters. The driver can query a machine learner with a set of
feature, and receive a decision. It can then use this decision to control
the compilation of a program unit.

@subsection GCC wrapper driver

MAGEEC offers one concrete interface to drive GCC for training and
optimization purposes which takes the form of a GCC wrapper. This
driver is a wrapper around the gcc/gfortran/g++ drivers, which
intecepts and controls the optimization flags passed to it.

When training, the driver tracks the optimization flags being used
with the driver and records a compilation in the database for each
program unit.

When optimizing, the driver strips optimization flags and instead
replaces the flags with flags derived through multiple calls out
to the MAGEEC interface to the machine learner. It uses previously
extracted features to inform this process.

Because the GCC wrapper driver works externally to the compiler, it
does not require any invasive changes to the compiler in order to
control the passes. It also supports multiple version of GCC easily
and can support Fortran and C++ code through the same driver.
Additionally because it uses the public documented interface to the
compiler, it is more stable as it cannot create broken pass pipelines <-- WUT? -->

A disadvantage of the GCC driver is that it is only able to make decisions on
a per-module level. If the driver instead operated internally to GCC it could
be possible to enable or disable passes on a per function basis. This
severely limits the optimization potential.

@c Flags vs internal pass decisions
@c High level diagram of compilation process for gather+optimize
@c @image Diagram of GCC wrapper driver process

@subsection MAGEEC file format

Internally MAGEEC uses an SQLite database as an intermediate format.
This MAGEEC database is the main file format, and is used to record
the intermediate stages of training. It holds extracted features,
records of each compilation and the accompanying parameters and features
for it, results, trained machine learners, and various debug information.

When the feature extractor is run, features are added to the database,
and a handle to an entry in the database is returned. Likewise,
when a program unit is compiled with a with a set of parameters,
a compilation is recorded in the database an an identifier to it
is returned. This compilation can then have a result value associated
with it later.

There are a few reasons for using an SQLite database as an
intermediate format:

@itemize
@item
The data by its nature contains relations which make it conducive
to using a database. By using a database, this relations can be
maintained and enforced.

@item
The database is a familiar conceptual model which is easier to
understand.

@item
sqlitebrowser can be used to query the database manually, making
it easier to debug and manually fix if it is ever necessary.

@item
SQLite allows multiple processes to read/write the same file,
which is useful for parallelizing the training process for
performance. Doing this with flat files would be more complex,
and require reimplementing functionality already offered by
SQLite.

@item
SQLite is quite high performance, and offers resilience against
corrupt data out of the box.
@end itemize

The main disadvantage to using SQLite is that it adds some
complexity to the interfaces, and requires more boilerplate code
to interface than a simpler flat file solution would.

@subsubsection Database structure

Below is a description of the file format structure and the
associated relations.

@c @image Database structure diagram

@subsection Machine learner interface

The machine learner interface provides the capability to train
a machine learner given a set of compilations and their results,
as well as make decisions using a previously trained machine learner.

When training, MAGEEC provides the machine learner with all of the
results data for all of the compilations which have been performed.
This associates a set of inputs features compiled using a particular
set of parameters to the compiler, with a single output result
value. The interface also provides a set of parameters which the
user may wish to make decisions on, and which therefore forms the set
of parameters which the machine learner should train to make a
decision against.

The machine learner can use the dump of data however it likes.
However, the end result of the training process is a single binary
blob which is opaque to MAGEEC and the MAGEEC file format, but
can be deciphered by the machine learner at a later time when it
is required to make a decision. This binary blob is returned by
the machine learner and then stored by MAGEEC in the database for
use later.

When optimizing, the blob for the machine learner is retrieved
and an instance of the machine learner is instantiated. The
user of the machine learner can then query the machine learner
to make decisions about parameters which the machine learner
encountered when training. The machine learner is provided the
decision to make, as well as a set of features to use in order
to make the decision. The machine learner can choose to make a
decision, or it can refuse, leaving it up to the user to then
make the decision itself.

The features provided to the machine learner are made up of a
unique identifier, as well as a 'type'. The unique identifier
is guaranteed by the feature extractor to unique identify the
same feature every time, and the type is one of a number of
types specified by MAGEEC.

The parameters which are provided to the machine learner also
have a unique identifier which is guaranteed to identify the
same parameter in the compiler which is being used. The
types of the parameters is also an identifier for a type specified
by MAGEEC.

@c Feature types parameter types, decisions?

Currently, there is no ability to do incremental learning.

@c @image Diagram of machine learner inputs and outputs for train + optimize

The machine learning interface is used by the standalone tool
in order to train a database based on generated results, and
it is used by the GCC wrapper driver in order to make decisions
during compilation.

In MAGEEC there currently exist two machine learners, a machine learner
based on the C5.0 classifier, and a machine learner based on a one
nearest-neighbor search. These are built as libraries which the
GCC wrapper then links against.

A user can create a new machine learner, provided it implements the
machine learner interface. It is then possible to use this machine
learner as a plugin to the standalone tool and GCC wrapper, by
building the machine learner as a dynamic library which the
tools can link against.

@subsubsection C5.0 Machine learner

C5.0 is a machine learning which creates a classifier tree based on
the input features. For MAGEEC, a C5.0 classifier tree is created
for each decision which may be made (corresponding to a parameter
in the compilation). The machine learner is provided the best
result for each distinct set of features, and it uses these
to build a classification tree for each parameter. The classification
tree for each of the parameters are concatenated together and
stored in the blob.

When a decision must be made, the appropriate classifier tree is
found, and the features for the program unit are used to determine
the classification for that decision. For a boolean decision, the
resulting decision could be 'true', 'false', or 'native' if the
machine learner is not able to make a decision.

@c @image Diagram of classifier trees for MAGEEC
@c @image Diagram of use of a classifer tree for MAGEEC

Advantages of C5.0 are that the interface is straightforward
and the decision trees are human readable.

@subsubsection 1-NN Machine learner

One nearest neighbor is a conceptually very simple classification
method. Each feature set form a point in n-dimensional space, where
each feature is mapped to an individual axis. Each feature set 'point'
is associated with a parameter set, which is the parameter set for
the best result for that feature set in the results data. When a
decision needs to be made, the closest feature set to the input
set is found in the training data, and the decision is made based
on the parameters in that closest point.

1-NN is a very simply classification technique, and is very easy
to understand. 

@c @image Diagram of 1NN classification for MAGEEC

@subsection Tooling

As well as the key MAGEEC interfaces and components, there are also
a few tasks which are achieved through tooling and python scripts.

@subsubsection Standalone MAGEEC tool

This tool exists to do a few tasks which can't be done by either
the feature extractor or compiler driver. This includes; adding
results into the database, training a database using a machine
learner; and creating and merging databases files. It is a simple
command line tool.

@subsubsection Feature extraction script

This script exists to build and feature extract a program using the
GCC feature extractor plugin. It simplifies the interface to the
plugin, and automatically handles autoconf and CMake build systems.

@subsubsection Combined elimination script

This script performs the combined elimination process as described in @c citation.
This script exists in order to do an iterative
search towards the best combination of compilation flags for a
benchmark, which can be used to effectively train many machine learners.
The naive way to find good flag combinations is to do a random
search, however combined elimination finds more optimal results in
far fewer runs. Even compared to doing a fractional factorial design
combined elimination is much more effective.

The combined elimination process starts with all flags enabled,
and then attempts to disable flags in groups in order to find a
more optimal set of flags for a given program. It has been shown
to be more effective than other iteractive compilation techniques.

@c @image Combine elimination diagram

Combined elimination is used by MAGEEC primarily for finding good
data points for the training process.

@subsection MAGEEC library

The various interfaces exposed by MAGEEC, and used by the GCC
feature extractor plugin, GCC wrapper driver and standalone tool
are all implemented in a core library which the MAGEEC tools,
and other software which wishes to use with MAGEEC can link
against. By including this in a library, it means that integration
with MAGEEC can be embedded into other compilers. This could
be used to perform feature extraction from within another
compiler, or to allow MAGEEC to alter the internal pass
sequences in a compiler.

@section Workflow

There are two phases to using MAGEEC. Gathering and training a
machine learner, and optimizing a program based on a trained
machine learner.

@subsection Gathering and training

The gathering process involves repeatedly building a number of
benchmarks under slightly different configurations of the
compiler. The resultant binaries can then be measured somehow,
and the results of that measurement can be associated
with the compilations of each of the program units which make
up the executable.

The aim of the gathering process is to find sets of parameters
which minimize the cost function (with the aim to beat the
default configurations of the compiler), and there are a few ways
to find this set.

The ideal approach would be to do an exhaustive search of the
parameter space to find the absolute best set of flags. If
possible, this would be ideal as you can guarantee that the
best result has been found. The problem with this approach
is that it is simply not possible to do an exhaustive search
of the parameter space. For a compiler such as GCC where there
are approximately 100 boolean optimization flags which can be
configured, there are 2^100 combinations which would need to
be tested.

Since an exhaustive search is not feasible, an alternative is
to only search a subset of the full search space. There are
several observation one might be able to make in order to
cut down the search space considerably:

@itemize
@item
In general, configurations with most flags on do better than
configurations with less flags. It is extremely unlikely
that enabling only a single flag would be better than -O3

@item
As a first approximation, most flags can be treated
independently.

@item
Many flags will have very little effect either way, and can
be excluded
@end itemize

Taking factors like this into account, it is possible to
trim down the search space considerably. Using techniques such
as fractional factorial design can be used to only test
a subset of combinations. In some cases, this is viable, however
for many programs used in the training process, compilation
times may be sufficiently high that it is worth looking for
better solutions.

In order to minimize the number of tests, incremental compilation
can be used in order to hill-climb towards better solutions.
There are various techniques that can be used for this, however
combined-elimination is straightforward and has been demonstrated
to find promising results.

Incremental compilation suffers from the risk that only a local
minima is found, however provided this local minima is still
an improvement over the default compiler behavior, this is
a trade-off worth making.

Another major component to the gathering process is the set
of benchmarks which are used. The benchmarks needs to be
representative of the types of programs which will be used
with the compiler. They also need to be varied, so that the
feature sets gather cover as many potential input programs as
possible, and most importantly they need to be numerous.
Machine learning requires a glut of data to produce reliable
results, so having a lot of benchmarks containing a lot of
modules and functions is important.

After the benchmarks have been run under a large number of
configurations, and they have been measured and their results
gathered into MAGEEC, they can be trained. Training involves
invoking the machine learner to produce a training blob
which is stored and used at a later point.

@subsection Optimizing

In comparison to gathering and training, the optimizing phase is
much simpler. Here a new program has its features extracted, and
these features are then passed to MAGEEC along with decisions
which need to be made. The decisions about parameters are made,
and this is used to inform the compilation of the program.

The result should be a binary which is more optimal than would
be produced by the default compiler.

@section Experimentation

@c Features
@c Machine learners
@c Optimizing for size
@c Optimizing for energy
@c NEALE

@c Cross-platform (in theory)
@c Flexible to different compilers (GCC implemented)
@c Flexible to different languages (C, C++, Fortran implemented)
@c Integrated or standalone (closed source compilers)
@c Library design
@c Based on flags or compiler internal decisions
@c Open source
@c Plugin interface for different machine learners

@c Justifications for decisions
@c Important terms
@c Combined elimination
@c Experiments
@c Results
@c Limitations
@c Future work
@c Cross platform
@c Multi-compiler
@c Based on flags or other
@c Multi-language (gfortran, gcc, g++)
@c NEALE/Hartree systems
@c Modules/Libraries
@c Gathering
@c Adding results + training
@c Workflow
@c Optimizing
@c Benchmarks (mantevo)
@c Feature quality
@c Diverse feature sets
@c Tools (gcc driver, standalone tool, feature extractor, scripts)
@c Documentation of tools and their flags
@c Considerations for benchmarking on supercomputers
@c Terms (Features/Parameters/Compilation/Program unit)
@c SQLite as an intermediate format
@c Minimize intermediate files
@c Structured, easy to debug (sqlitebrowser, sqlite3)
@c Simple conceptual model
@c Multiprocess access and good performance
@c Allinea MAP
@c Machine learners
@c C5.0
@c 1NN
@c Design decisions
@c Open source

@node GNU Free Documentation License
@chapter GNU Free Documentation License
@include fdl-1.3.texi

@bye
