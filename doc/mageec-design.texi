\input texinfo
@setfilename mageec-design.info
@afourpaper
@settitle MAGEEC Design
@paragraphindent 0

@copying
This design document is for MAGEEC.
This manual is for MAGEEC.

Copyright @copyright{} 2015 Embecosm Limited.
@quotation
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, with no Front-Cover Texts, and with no Back-Cover
Texts. A copy of the license is included in the section entitled
``GNU Free Documentation License''.
@end quotation
@end copying

@titlepage
@title MAGEEC Design
@subtitle MAchine Guided Energy Efficient Compilation
@author Edward Jones
@page
@vskip 0pt plus 1filll
@insertcopying
@end titlepage

@contents

@node Top
@top Scope of this Document
The document outlines the design of MAGEEC @emph{MAchine Guided Energy
Efficient Compilation}.

@menu
* MAGEEC Design::
* References::
* Glossary::
* GNU Free Documentation License::
@end menu

@node MAGEEC Design
@chapter MAGEEC Design

@section Overview

MAGEEC is a set of components to augment a compiler with the ability
to make intelligent predictions about which optimizations to run based
on a machine learner trained against statically extracted features of
the program under compilation.

Contemporary compilers mostly use a fixed pipeline of passes when
optimizing. This pipeline can often be configured by the user, either
at a high level to specify the overall degree of optimization desired,
or at a low level to enable or disable specific optimizations, however
for a given invocation of the compiler the pipeline is for the most
part fixed. The pass pipelines for common optimization levels (such
as -O3 in gcc) are balanced to work well with the widest range of
code, however for a given piece of code the passes will not necessarily
be optimal. In many compilers the user may tune the pipeline for a
given program or source file, however this is generally a involved
manual task which requires trial and error, and a lot of iterations
of compilation.

@center @image{"images/current-pass-pipelines", 150mm}

From the perspective of the user, it would be reassuring to know
that the compiler is not missing optimization opportunities. If the
compiler could be relied on to make more accurate optimization decisions
it could also be feasible to simplify the user experience by
not requiring them to manually disable and enable flags to
squeeze maximum performance from their code.

From the perspective of the compiler writer, it would be beneficial
to avoid the laborious task of tuning the pass pipeline for each new
processor and CPU. For a given architecture, there can exist a vast
array of CPUs which vary in the hardware features supported and
the intricacies of their microarchitecture. For example, for the
AArch64 backend of LLVM, there are currently 15 different CPUs
which can be targeted, differing in the set of supported features
and the scheduling model. For each CPU it is necessary to optimize
the compiler to target it, and automatically optimizing the pass
pipeline is a way in which extra performance may be extracted for
a given core with minimal input from the compiler writer.

A further point of interest is that most compilers currently
optimize for speed, and give less attention to code size, energy,
or any other interesting metrics. In general optimization can only
be done with one goal (typically speed or code size) and it is not
possible to express more complex optimizations goals such as
optimizing for energy and code size at the same time, or
optimizing for speed whilst keeping compile time to a minimum.

If selecting the best compiler configuration can be automated,
then there are several potential benefits which may be realized:
  
@itemize
@item
Remove complexity from the compiler to tune optimization
pipelines for different ISAs, microarchitectures, CPU features
sets, and metrics.

@item
Provide the user a more straightforward interface, as well
as better performing programs.

@item
Control optimizations at a finer granularity (file, module) to
realize performance improvements which cannot be realized in current
compilers (which can only control the pass pipeline at a
per-invocation level).

@item
Optimize for new metrics such as energy or compilation speed.

@item
Provide a more powerful interface to control tradeoffs
between different metrics, such as balancing energy and size.
@end itemize

@center @image{"images/better-pass-pipelines", 150mm}

@section Other optimization approaches

As it stands, most compilers appear to use a fixed pipeline of
passes when optimizing. However there has been other work to make
pass control more powerful. Some of these techniques exist in
contemporary compilers, however many only appear in research.

@itemize
@item
@b{Profile-guided Optimization} - This is common in many open source
and propreitary compilers now. Optimization is tuned based on
profiling data from running the program with representative
data. The profile provides information about the hot-spots in
the code, which can be used to inform the ordering of basic
blocks and the ferocity of inlining.

Profile guided optimization can produce considerable
performance improvements (in some cases as much as 15% improvement).
However there are a few drawbacks. First, it requires a
project to be built twice which can be cumbersome for larger software
projects. Second, it requires a long run of the profiled
program with representative data to produce an accurate
profile. Finally, current PGO implementations only provide a
limited amount of information about the program; for example
in GCC using the -fprofile-generate option will generate
a profile which stores information about branch probabilities,
and the values of some expressions in the program [1].

@item
@b{MILEPOST} - Much like the aim of MAGEEC, MILEPOST augmented
a compiler with the ability to make optimization decisions
based on statically derived features of an input program. [2]

MAGEEC tries to improve on MILEPOST by providing a more
flexible framework to augment compilers with a machine
learner, as well as also targeting energy as an
optimization metric.

Whereas the MILEPOST project used a custom version of GCC
4.2, MAGEEC offers a way to interface with multiple
compilers, including compilers which don't have a plugin
interface. Equally, MAGEEC provides a way to interface
with multiple machine learners whereas MILEPOST used a
single machine learner technique called Predictive
Search Distributions.

@item
@b{Other Examples}
@c TODO: Other examples
@end itemize

@section Terminology

@itemize
@item
@b{Program Unit} - A contiguous part of a program which can be uniquely
identifier in both the original source code and the final executable.
Examples are modules, functions, loops and basic blocks.

@item
@b{Feature} - A measurable and quantifiable aspect of a program unit, which
represents an aspect of the program unit in some way. For example,
the number of instructions in a basic block.

@item
@b{Parameter} - A value which can be controlled by some mechanism in the
compiler to affect the decree or nature of optimization, with the
effect of changing the object code produced by the program. For example:
a flag indicating whether the `dead code elimination' pass should be
executed.

@item
@b{Compiler configuration} - A set of parameters used to compile an
individual program unit. Multiple units of the same program may
use the same configuration of the compiler.
@end itemize

@section Theory behind MAGEEC

MAGEEC automates the selection of optimizations by using a
machine learner to make key decisions about the pass to
run and any tunable parameters which may be set. An
association is learned between features derived from the
input source code, and the appropriate parameters to use in
the compilation to build the executable. Like all machine
learning processes, there is a training stage; where the
machine learner is fed features and corresponding `good'
parameters to build up an association; and a prediction stage;
where the machine learner is used with a new, previously
unencountered program and used to select a set of parameters
which are likely to produce a better performing output executable.

@center @image{"images/high-level-gather", 150mm}

@center @image{"images/high-level-predict", 150mm}

@section Comparison with other compiler tuning approaches

MAGEEC uses static features derived ahead of time from the input
program in order to predict parameters which should be used
for a given compilation. However there are a couple of other
approaches which can be used for compiler tuning.

One approach is to predict compiler parameters based on
dynamic features instead of static features. Where static
features are features derived at compile time from the input
source code, dynamic features are derived at run time based
on instrumentation of the tested program.

@c citations for other work here?

Another approach is to iteratively compile a program gradually
refining the set of compilation
parameters after each build. Compared these approaches,
our approach based on a predictive model using static features
has some advantages and disadvantages:

Advantages:
@itemize
@item
To derive static features, test programs do not have to be recompiled to
receive a benefit. Interactive compilation techniques such as combined
elimination require a large number of recompilation runs, and even just
doing a single profiled run to derived dynamic features can be
prohibitively difficult if the program is large or cannot easily be
instrumented.

@item
Compared to using dynamic features, measuring static features is simpler.
Dynamic features may not be precisely measureable, may depend on the
environment, or require a more complex build stage.

@item
Measuring static features is fast. The effect on compilation time can
be minimized as static features can be very easily derived from the
source code or intermediate representation. This makes deployment
far easier.

@item
In some cases it may not be possible to instrument and measure dynamic
features. For example, an embedded system may not have enough memory
or storage for the overhead of intrumentation.

@item
Instrumentation of the program in order to measure dynamic features
of the program can affect the behavior of the program, and can
affecting the measurement of those features. Static features do not
alter the program under measurement in any way.

@item
If dynamic features are measured, then and large amount of
representative input needs to be used in order to accurately
quantify the behavior of the program. Producing this data may
be a non-trivial task itself.
@end itemize

Disadvantages:
@itemize
@item
Static features cannot express the effects of the input data on the
program. Two program can have identical features, but exhibit very
different performance characteristics depending on the data provided
to it. Indeed, one program on its own can also exhibit very different
performance depending on the data provided to it. This can be mitigated,
as the features of the program will often hint at the type and patterns
of data usage, however the problem cannot be fully mitigated.

@item
Static features may be less representative of the program's behavior.
For example a program may contain a lot of code which is only executed in
exceptional circumstances. If dynamic features or interative
compilation is used, then these cold code paths may never be exercised,
and therefore won't bias the features. If static features
are used, then these cold paths will be equally represented in the
features as the hot code, even though they are almost irrelevant to
the performance of the program.
@end itemize

@section Features of MAGEEC

The design of MAGEEC makes it possible to use a variety of machine
learners through a common interface, and makes it possible to use
both command-line driven proprietary compilers, and open source
compilers. The interface to MAGEEC can either be a wrapper around
the compiler (ideal for a proprietary compiler), or can integrate
in the compiler itself, possibly through a plugin interface. Deeper
integration is beneficial as it may allow compilation controlling
decision to be made at a finer granularity than program or module
level.

@c Diagram of internal/external MAGEEC control

MAGEEC is open source under the GPL license. The aim is that by doing
this MAGEEC can be freely implemented in as wide range of compilers as
possible.

@c @section Comparison with original MAGEEC project
@c
@c The original MAGEEC project aimed at optimizing energy consumption
@c on embedded systems. The aim of this updated was to additionally
@c target high performance applications as well as improve the workflow
@c to make using MAGEEC more straightforward for users.

@section MAGEEC components

MAGEEC itself is made up of a set of components and utilities which
allow a compiler to be instrumented with the ability to predict
good parameters to use in the compilation using machine learners.
The components in MAGEEC include:

@itemize
@item
@b{Feature Extractor} - An interface to derive static features
from an input program, as well as an implementation of a feature
extractor for GCC.

@item
@b{Compiler driver} - A mechanism to drive a compiler to exercise
various parameters during the compilation phase. Currently this is
implemented for GCC as wrapper around the GCC frontend which can
alter optimization flags on a module-by-module basis.

@item
@b{File Format} - A file format to record features, compilations
and results for programs units in a centralized place. This is
implemented through an SQLite3 database.

@item
@b{Machine Learner Interface} - An interface which can be
implemented by machine learners to be used with the framework.
In addition to this, basic machine learners are provided in the
form of the C5.0 classifier and a 1NN machine learner.

@item
@b{Tooling} - Tools and scripts to access the file, and drive
the training and prediction process.
@end itemize

@center @image{"images/high-level-components", 150mm}

We now describe each of these components in more detail, and
their current implementations.

@subsection Feature extraction

Feature extraction involves taking an input source program, and
outputting a set of features for each of the individual
units of the program. Each source file can produce
multiple sets of features for each different program unit in 
the file. So the module will produce a feature set, and each
function will produce its own feature set. It is even feasible
that feature could be generated for each basic block or loop.

@subsubsection Interface

MAGEEC provides an interface to create a set of features. How
exactly the features are extracted is up to the program doing the
feature extraction, and the features which are extracted is also
at the whim of the feature extractor. The limitations placed by
MAGEEC are that the type of the features must be one of a set of
known types, which is a set of types which is common with the types
which the machine learners can handle, and a given extracted feature
is consistently identified by a numerical identifier.

The interface to MAGEEC allows a feature extractor to insert a
new set of features into the file for a program unit, and
receive a unique identifier for those features back.

@subsubsection GCC plugin

@center @image{"images/feature-extractor", 150mm}

As well as the interface to the feature extractor, a feature
extractor plugin is also provided for the GCC compiler. This
plugin can be provided as a plugin to GCC and used to
extract module and function features from the program.

This plugin takes the input program and outputs a .csv file
identifing an individual program unit in the source code, associating
it with the unique identifier of the feature set for that
program unit.

The benefit of the GCC feature extractor is that it allows feature
extraction to be performed on C, C++ and Fortran code without
requiring a compiler-specific feature extractor to be written.
Provided code can be built with GCC, feature extraction can be
achieved. This makes it ideal for use alongside a proprietary
compiler which cannot be augmented with feature extraction itself.

The feature extractor extracts different sets of features
depending on whether a function or a module is the program unit
under compilation. The features extracted for modules are for
the most part just aggregated values based on the values for
each function in that module.

@xref{GCC plugin features}

@subsection Compiler driver

In order to drive the training and prediction process, there needs
to be a mechanism to control the parameters provided to the compiler.
When training, it needs to be possible to run the compiler under a
number of controlled configurations, and record each of these
compilations in the MAGEEC database for later use. During prediction
it needs to be possible to call out to MAGEEC to make decisions using
a trained machine learner and then use these decisions to control
compilation parameters.

@subsubsection Driver interface

MAGEEC offers an interface, which allows the compiler, or a wrapper
controlling the compiler to store a record of each program unit
being compiled, the features associated with that program unit, and
parameters describing the configuration of the compiler for that
compilation. The interface takes all of the information describing
the compilation of a program unit; the features and parameters
involved in the compilation, and returns a unique identifier for
that compilation. Later, results will be associated with this
compilation and the features, parameters and results used in a
given compilation will be used as a data point as input to training
the machine learner.

For prediction, an interface is also offered which allows the driver
to use a previously trained machine learner to make decisions about
parameters. The driver can query a machine learner with a set of
features as well as a parameter to make a decision about. It then
receives a decision which it can use to inform its choice about
the value the parameter should take for the compilation of a 
program unit.

@subsection GCC wrapper driver

MAGEEC offers one concrete interface to drive GCC for training and
prediction purposes which takes the form of a wrapper around the
GCC frontend. This wraps around the gcc, gfortran and g++ drivers
in order to intercept and control the optimization flags passed
to those commands.

@center @image{"images/compiler-driver-gather", 150mm}

When training, the driver tracks the optimization flags being used
with the driver and records a compilation in the database for each
program unit.

@center @image{"images/compiler-driver-predict", 150mm}

When predicting, the driver strips optimization flags and instead
replaces the flags with flags derived from predictions made by the
machine learner accessed through MAGEEC. The prediction makes use
of the features previously extracted for the current module.

Because the GCC wrapper driver works externally to the compiler, it
does not require any invasive changes to the compiler in order to
control the passes. It also supports multiple version of GCC easily
and can support Fortran and C++ code through the same driver.
Additionally because it uses the public documented interface to the
compiler, it is more stable as it should not be able to create
an invalid set of passes.

A disadvantage of the GCC driver is that it is only able to make decisions on
a per-module level. If the driver instead operated internally to GCC it could
be possible to enable or disable passes on a per function basis. This
severely limits the optimization potential.

@c more detail on this bit

@c Flags vs internal pass decisions

@subsection MAGEEC file format

Internally MAGEEC uses an SQLite database as an intermediate format.
This MAGEEC database is the main file format, and is used to record
the intermediate stages of training. It holds extracted features,
records of each compilation and the accompanying parameters and features
for it, results, trained machine learners, and various debug information.

When the feature extractor is run, features are added to the database,
and a handle to an entry in the database is returned. Likewise,
when a program unit is compiled with a with a set of parameters,
a compilation is recorded in the database an an identifier to it
is returned. This compilation can then have a result value associated
with it later.

There are some benefits to using an SQLite database as a
storage format:

@itemize
@item
The data by its nature contains relations which make it conducive
to using a database. By using a database, these relations can be
maintained and enforced.

@item
The open source sqlitebrowser tool [3] can be used to run SQL queries
agains the database manually, making it easier to debug and
manually fix if it is ever necessary.

@item
SQLite allows multiple processes to read/write the same file,
which is useful for parallelizing the training process for
performance

@item
SQLite is high performance, and offers resilience against
data corruption.
@end itemize

The main disadvantage to using SQLite is that it adds some
complexity to the interfaces. However this is worthwhile
for the added flexibility which it offers.

@subsubsection Database structure

Below is a description of the database structure and the
associated relations.

@center @image{"images/database", 150mm}

@itemize
@item @b{Metadata} - This table stores various metadata
about the database itself. The only value this is used for
is the version number of the database.

@item @b{FeatureType} - The feature extractor has a number
of distinct features of the input program which are extracted.
Each of these is assigned a unique identifier. This table
maps from this identifier to an enumeration value which
identifies the type of the feature. The valid `type' enumeration
values are specified by the framework.

@item @b{FeatureSetFeature} - This stores a one-to-many mapping
from an identifier for a set of features, to the features which
make up that feature set. A FeatureSet is made up of multiple
features, where each feature is represented by an identifier
from the feature extractor which identifiers the feature, and
a value for the feature.

@item @b{ParameterType} - The driver of the compiler defines
a number of parameters which can be tweaked or altered and
assigns each one a unique identifier. For example, a boolean
parameter could represent the presence or abscence of an
optimization flag. Like the FeatureType, this stores a mapping
from this identifier to an enumeration value which identifies
the type of the parameter. Like Features, the valid `type'
enumeration values are specified by the framework.

@item @b{ParameterSetParameter} - This stores a one-to-many
mapping from an identifier for a set of parameters, to the
parameters which make up that parameter set. Parameters in the
parameter set have the same form as a features, with an
identifier and a value.

@item @b{Compilation} - For each program unit which is
compiled, an entry in the compilation table is created.
Entries in the Compilation table create an association
between a FeatureSet and a ParameterSet. The FeatureSet
describes the features in the program unit which is being
compiled, and the ParameterSet describes the compiler
configuration that FeatureSet was compiled under. Each
Compilation is identified by a unique identifier, and
after the Compilation entry has been created this is used
to associate results with the compilation.

@item @b{Result} - A result represents a value measured
for some program unit. It is linked to an entry in the
Compilation table, which represents the program unit and
the configuration of the compiler when it was compiled,
and associates this with a value which quantifies the
result of the compilation of that program unit. A result
has an associated metric, which identifies what quality
of the program unit was measured.

@item @b{MachineLearner} - An entry in this table is
created when a machine learner is trained. The
data in the Result table is used by a machine learner
in order to generate training data, and this training
data is then stored as a blob in an entry in the
MachineLearner table. Like individual results, each
entry has an associated metric, and this describes the
results which were used to train the machine learner.

@item @b{CompilationDebug} - This stores metadata about
each compilation.

@item @b{FeatureDebug} - This stores metadata about
each feature.

@item @b{ParameterDebug} - This stores metadata about
each parameter.
@end itemize

@subsection Machine learner interface

The machine learner interface provides the capability to train
a machine learner given a set of compilations and their results,
as well as make decisions using a previously trained machine learner.

When training, MAGEEC provides the machine learner with all of the
results data for all of the compilations which have been performed.
This associates a set of inputs features compiled using a particular
set of parameters to the compiler, with a single output result
value. The interface also provides a set of parameters which the
user may wish to make decisions on, and which therefore forms the set
of parameters which the machine learner should train to make a
decision against.

The machine learner uses the results data in order to generate
some training data. The end result of the training process is a single binary
blob which is opaque to MAGEEC and the MAGEEC file format, but
can be deciphered by the machine learner at a later time when it
is required to make a decision. This binary blob is returned by
the machine learner and then stored by MAGEEC in the database for
use later.

When predicting, the blob for the machine learner is retrieved
and an instance of the machine learner is instantiated. The
user of the machine learner can then query the machine learner
to make decisions about parameters which the machine learner
encountered when training. The machine learner is provided the
decision to make, as well as a set of features to use in order
to make the decision. The machine learner can choose to make a
decision, or it can refuse, leaving it up to the user to then
make the decision itself.

The features provided to the machine learner are made up of a
unique identifier, as well as a `type'. The unique identifier
is guaranteed by the feature extractor to unique identify the
same feature every time, and the type is one of a number of
types specified by MAGEEC.

@c Why is it specified by MAGEEC

The parameters which are provided to the machine learner also
have a unique identifier which is guaranteed to identify the
same parameter in the compiler which is being used. The
types of the parameters is also an identifier for a type specified
by MAGEEC.

@c Why is it specified by MAGEEC

@c Feature types parameter types, decisions?

Currently, there is no ability to do incremental learning.

@c Why not possible to do incremental learning

@c @image Diagram of machine learner inputs and outputs for train + predict

The machine learning interface is used by the standalone tool
in order to train a database based on generated results, and
it is used by the GCC wrapper driver in order to make decisions
during compilation.

In MAGEEC there currently exist two machine learners, a machine learner
based on the C5.0 classifier, and a machine learner based on a one
nearest-neighbor search. These are built as libraries which the
GCC wrapper then links against.

A user can create a new machine learner, provided it implements the
machine learner interface. It is then possible to use this machine
learner as a plugin to the standalone tool and GCC wrapper, by
building the machine learner as a dynamic library which the
tools can link against.

@subsubsection C5.0 Machine learner

C5.0 is a machine learning which creates a classifier tree based on
the input features. For MAGEEC, a C5.0 classifier tree is created
for each decision which may be made (corresponding to a parameter
in the compilation). The machine learner is provided the best
result for each distinct set of features, and it uses these
to build a classification tree for each parameter. The classification
tree for each of the parameters are concatenated together and
stored in the blob.

When a decision must be made, the appropriate classifier tree is
found, and the features for the program unit are used to determine
the classification for that decision. For a boolean decision, the
resulting decision could be `true', `false', or `native' if the
machine learner is not able to make a decision.

@center @image{"images/c50", 150mm}

Advantages of C5.0 are that the interface is straightforward
and the decision trees are human readable.

@subsubsection 1-NN Machine learner

One nearest neighbor is a conceptually very simple classification
method. Each feature set form a point in n-dimensional space, where
each feature is mapped to an individual axis. Each feature set `point'
is associated with a parameter set, which is the parameter set for
the best result for that feature set in the results data. When a
decision needs to be made, the closest feature set to the input
set is found in the training data, and the decision is made based
on the parameters in that closest point.

@center @image{"images/1nn", 150mm}

@subsection Tooling

As well as the key MAGEEC interfaces and components, there are also
a few tasks which are achieved through tooling and python scripts.

@subsubsection Standalone MAGEEC tool

This tool exists to do a few tasks which can't be done by either
the feature extractor or compiler driver. This includes; adding
results into the database, training a database using a machine
learner; and creating and merging databases files. It is a simple
command line tool.

@subsubsection Feature extraction script

This script exists to build and feature extract a program using the
GCC feature extractor plugin. It simplifies the interface to the
plugin, and abstracts away building of projects with autoconf and
CMake build systems.

@subsubsection Combined elimination script

This script performs the combined elimination process as described by Pan
and Eigenmann [4]. This script exists in order to do an iterative
search towards the best combination of compilation flags for a
benchmark, which can be used to effectively train many machine learners.
The naive way to find good flag combinations is to do a random
search, however combined elimination finds more optimal results in
far fewer runs. Even compared to doing a fractional factorial design
combined elimination is much more effective.

The combined elimination process starts with all flags enabled,
and then attempts to disable flags in groups in order to find a
more optimal set of flags for a given program.

@c Combined elimination pseudo code

Combined elimination is used by MAGEEC primarily for finding good
data points for the training process.

@subsection MAGEEC library

The various interfaces exposed by MAGEEC, and used by the GCC
feature extractor plugin, GCC wrapper driver and standalone tool
are all implemented in a core library which the MAGEEC tools,
and other software which wishes to use with MAGEEC can link
against. By including this in a library, it means that integration
with MAGEEC can be embedded into other compilers. This could
be used to perform feature extraction from within another
compiler, or to allow MAGEEC to alter the internal pass
sequences in a compiler.

@section Workflow

There are two phases to using MAGEEC. Gathering and training a
machine learner, and predict optimizations for a program based
on a trained machine learner.

@subsection Gathering and training

The gathering process involves repeatedly building a number of
benchmarks under slightly different configurations of the
compiler. The resultant binaries can then be measured somehow,
and the results of that measurement can be associated
with the compilations of each of the program units which make
up the executable.

The aim of the gathering process is to find sets of parameters
which minimize the cost function (with the aim to beat the
default configurations of the compiler), and there are a few ways
to find these parameter sets.

The ideal approach would be to do an exhaustive search of the
parameter space to find the absolute best set of flags. If
possible, this would be ideal as you can guarantee that the
best result has been found. The problem with this approach
is that it is simply not possible to do an exhaustive search
of the parameter space. For a compiler such as GCC where there
are approximately 100 boolean optimization flags which can be
configured, there are 2^100 flag combinations which would need
to be tested.

Since an exhaustive search is not feasible, an alternative is
to only search a subset of the full search space. There are
several observation one might be able to make in order to
cut down the search space considerably:

@itemize
@item
In general, configurations with most flags on do better than
configurations with less flags when optimizing for energy and
time. It is extremely unlikely that enabling only a single
flag would be better than -O3

@item
As a first approximation, most flags can be treated
independently.

@item
Many flags will have very little effect either way, and can
be excluded
@end itemize

Taking factors like this into account, it is possible to
trim down the search space considerably. Using techniques like
principle component analysis and fractional factorial design
to create a training set it is possible to reduce the amount
of test runs that need to be done substantially. In some cases,
this approach is viable, however for many programs used in the
training process compilation times may be sufficiently high
that it is worth looking for better solutions.

In order to minimize the number of tests, incremental compilation
techniques can be used in order to hill climb towards solutions
which beat the most aggressive optimization options like -O3.
There are various techniques that can be used for this, however
combined elimination is straightforward and has been demonstrated
to find promising results. In the paper introducing combined
elimination, Pan and Eigenmann saw average improvements of
4.1% over -O3 for SPEC CPU2000 floating point benchmarks, and
3.9% for integer benchmarks [4].

Many incremental compilation techniques suffer from the risk
of only finding a local minima, rather than a global minima, and
this is true of combined elimination too. Provided the local
minima is still an improvement over the default compilation
optimizations this is not a major concern, and is a trade-off
working making in order to reduce the search time.

Aside from the process used to guide the search, another
important component is the set of benchmarks which are used.
The benchmarks need to be representative of the types of
programs which will be used with the compiler. They also
need to be varied so that the features gathered for
the benchmarks are as representative as possible of
the features which will manifest in any potential real-world
programs. Most importantly though is that there need to
be a lot of them. Machine learners require a glut of
data to produce reliable and predictable results, so having
a lot of benchmarks with as many program units (modules and
functions) as possible is key.

After the benchmarks have been run under a large number of
configurations, the quality of the resultant executables
can be measures and the results gathered in MAGEEC. After
results have been gathered, machine learners can be
trained. Training involves invoking a machine learner with
the results data in order to produce a blob of training
data which can be stored to be used when predicting optimizations
at a later point.

@subsection Prediction

In comparison to gathering and training, the prediction phase
is very straightfoward. Here a previously unseen program has its
features extracted, and these features are passed to a machine
learner by the compiler through MAGEEC. The features are
accompanied by a decision to make, which is a parameter which
the machine learner was previously trained to make decisions
against. This parameter could correspond to an optimization
flag to be enabled or disabled, or an internal heuristic value
to be tuned. The machine learner will provide a decision to
the compiler, and the compiler can use this to update the
parameters which will be used to compile the program unit.

The result of this process should be a binary which is more
optimal than the binary which would have been produced using
the standard optimization options.

@section Experimentation

The previous sections have described the theory and design of
MAGEEC. Now we describe an experiment designed to test the
framework.

@subsection Motivation

Current compilers generally optimize for speed, and
occasionally for size. One of the advantages of the
approach we have proposed is that it makes it possible
and easy to target any other metric which is quantifiable.

One of the key goals of MAGEEC is to optimize to reduce
the energy consumption of programs. The current approach
to optimizing for energy is generally based on the assumption that
speed and energy usage are closely intertwined, and
therefore simply optimizing for speed is sufficient.
Previous work by Pallister et al [5] verified that this was
broadly true, however they also showed that for architectures
with more complex pipelines such as the Cortex-M3, the
correlation was not as strong. MAGEEC seeks to build on this
observation to make it possible to automatically specialize
a compiler's configuration to reduce energy consumption
specifically.

Presently, much work is going into the task of reducing
energy consumption. This is important in multiple fields.
In embedded systems, this translates into more computational
power from smaller devices; and in HPC this can result in
reduced electricity bills. Reducing energy consumption can
also allow for new technology to be made viable. For example
a reduction in energy consumption used by a processor may
mean that it is viable to power it using solar energy.

@c First reference to the 'original MAGEEC project'
The original MAGEEC project looked at reducing energy
consumption in embedded systems. This project has built on
this work and aims to demonstrate that this is also
applicable to high performance computing applications. HPC
systems are used extensively for research, where
large amounts of computing resources are needed to performance
massive distributed simulations. Building a modern supercomputer
is expensive, however the energy cost of running a supercomputer
also forms a substantial cost of the system over its lifetime.
Any steps which reduce the energy consumption of the system
would therefore either reduce the energy bill of running the
system, or allow a more powerful system to be built in the
same power budget.

@c statistics

@subsection Metrics

MAGEEC was designed to be useable with any metric, however
the aim of the project is to reduce energy consumption
for HPC application, therefore the energy metric was
targeted in experiments.

Energy poses some unique challenges to measure
accurately, and is harder to achieve than measuring
program speed.

The first barrier when measuring energy is obtaining
accurate results, as there are many ways in which
energy values can become skewed:

@itemize
@item Scheduling of processes by the operating system can
cause variations betweens, as the process may be preempted
or moved between runs.

@item Modern processor dynamically change their clock
frequency which affects energy consumption in unpredictable
ways.

@item The physical core which benchmarks run on may
differ slightly, causing measurement error.

@item NUMA and inter-process communication energy usage
may vary considerably depending on which physical cores
a given process is executed on.

@item Sampling of energy results will have some effect
on the execution of the program.

@item A sampling rate which is too coarse may produce
an inaccurate profile of the program energy usage.
@end itemize

Some of these problems can be mitigated. For example on
many systems it may be possible to clock the clock
speed of processor cores. Equally, it is possible to
minimize the number of other processes running on
many systems to reduce interruption caused by the
sharing of resources. Indeed on HPC systems the
environment in which a program is run can be controlled
to the extend that it is possible to get accurate
energy measurement results
@c Quantification of accuracy of energy measurement on HPC

A second difficulty is accurately associating sampled
results with program units in the input program. The aim
of optimizing for energy is to reduce the amount of
energy spent when executing a function in the program.
This requires an accurate average for how much energy
was spent for each invocation of that function. However
when measuring energy consumption through sampling, we
can only determine the total amount of energy spent in a
function throughout the run of the program. Therefore,
to obtain the average amount of energy per function
invocation we must take the total and divide it by the
number of calls to that function. This then gives us
an estimate for the average energy used during each
invocation of the function.

@c Describe per function energy measurement better

For our purposes, we are running on a machine running
at a fixed frequency, with no other competeting
processes. Therefore, we can be reasonably confident
in the accuracy of our energy measurement. In order
to extract energy measurements, we use the Allinea
MAP framework.

@c Quantify accuracy of the energy measurement

@subsubsection Allinea MAP

Allinea MAP is a profiler for C/C++ and Fortran code.
It is able to profile OpenMP, MPI and pthreads code
with minimal overhead, which makes it ideal for
profiling HPC codes. As well as offering a user interface,
it can also output profiles in various formats
which can be used by other tools.

@subsection Benchmarks

@c segue

For use when training, there are some desirable properties
which should be covered by the benchmark:

@itemize
@item
The benchmark needs to containg a large number of program
units. This means that the benchmark should containing a
lot of modules and functions. Each module and function
will provide a single distinct feature set, and the number
of feature sets needs to be maximized in order to improve
the quality of any training.

@item
The benchmark should be straightforward to build. It
needs to be possible to pass flags to the compiler in
order to control the compile options, and
this requires the build system to not get in the way.

@item
The compilation time should be low, as a long compile time
causes the training process to take longer.

@item
It must be straightforward to instrument and run the
resultant programs to derive results. For some metrics
such as code size it is not necessary for the produced
executable to be run, but for metrics
like energy it is important to be able to run under
a profiler in order to gather energy measurements.

@item
Representative input data. The input data used when
measuring the program must be representative of the
kind of data which would be provided to a similar
real-world program.

@item
Controllable run time. In order to accurately take
time and energy measurements it may be necessary to
run for a long time under a profiler. If the program
does not run for sufficiently long then results will
be inaccurate, if it runs for too long then it will
increase the time to gather results considerably.

@item
Representaive code for the target system.

@item 
Minimal external dependencies. Any time or energy
spent in external libraries cannot be associated with
any compiled program unit and therefore can't be
used by the machine learner or help with training.

@item
Minimal disk access. The amount of disk access
cannot be realistically affected by changing compilation
parameters, so any time or energy spent on disk
or file access will only serve to skew result values.

@end itemize

These requirements restrict the pool of available
code considerably. For our experiments, we sought
to optimize for energy on HPC systems, so the
benchmarks needed to be representative code to
run on HPC systems.

In the end, we settled on using a number of benchmarks
from the Mantevo benchmark set.

@subsubsection Mantevo

Mantevo is a set of small benchmarks which contain
common numerical kernels used in HPC applications [6].
They have minimal external dependencies and the applications
are build with simple Makefiles.

The benefits of using Mantevo are:
@itemize
@item 
Very representative code for common HPC applications

@item
A good range of different HPC codes

@item
Each program is relatively self contained and makes minimal
use of external libraries.

@item
The programs are non-trivial and contain a reasonable number of
modules and functions.

@item
A mixture of Fortran, C and C++ code

@item
The build systems for each program are simple makefiles, and
can be easily adjusted if necessary.
@end itemize

The main disadvantage of using Mantevo is that there are only 13
miniapps in total.

The applications in the current version of Mantevo (version 3.0) are
as follows:

@multitable @columnfractions .25 .75
@item CloverLeaf @tab A Lagrangian-Eulerian hydrodynamics benchmark @c cite
@item CloverLeaf3D @tab A 3D Lagrangian-Eulerian hydrodynamics benchmark @c cite
@item CoMD @tab A reference implementation of classic molecular dynamics algorithms @c cite
@item HPCCG @tab High performance conjugate gradient benchmark @c cite
@item MiniAero @tab Solver for the compressible Navier-Stokes equations @c cite
@item MiniAMR @tab Adaptive mesh refinement @c cite
@item MiniFE @tab Finite-element code @c cite
@item MiniGhost @tab 3D nearest neighbor halo-exchange communication @c cite
@item MiniMD @tab Molecular dynamics solver @c cite
@item MiniSMAC2D @tab 2D incompressible Navier-Stokes algorithm @c cite
@item MiniXyce @tab Linear circuit simulator @c cite
@item Pathfinder @tab Graph search @c cite
@item TeaLeaf @tab Linear heat conduction equation solver @c cite
@end multitable

@subsection NEALE

The testing of the MAGEEC framework was performed on a system
called NEALE, a novel cooling demonstrator which is part of the
Energy Efficient Computing Research Program at the STFC Hartree
Centre [7]. The specification of this system are as follows:

@multitable @columnfractions 0.25 .75
@item Architecture @tab Ivybridge
@item Number of nodes @tab 120
@item Node configuration @tab 2x 8 core Intel Xeon E5-2650v2xi 2.6GHz, 64GB RAM per node
@item Total cores @tab 1920
@item Total memory @tab 7.5TB
@item Interconnect @tab QLogic (QDR-80)
@item Storage @tab 216TB BeeGFS parallel storage
@end multitable

The system uses the SLURM batch scheduler, and has support
for profiling through Allinea MAP.

The Ivybridge Xeon is a common processor used in many HPC systems and
the large number of nodes makes it possible to run a large
number of benchmark test runs in parallel.

@subsection Experiment setup

Here, we describe the setup of our experiment using the MAGEEC
framework.

The NEALE machine is used to compile and run all of the
benchmarks, and energy measurements are taken using Allinea MAP.
The MAGEEC GCC feature extractor plugin is used to extract
features from the benchmarks, and the GCC wrapper is
used to record and control the pass pipeline which is
being run. The underlying version of GCC used was
4.8.5.

@c scripts

There are two phases to the experiment; a training phase
and a prediction phase. the steps in each of these phases
is as follows:

@subsubsection Training phase

During the training phase, the high level steps are as
follows:

@itemize
@item
Interactively compile a subset of Mantevo benchmarks using
the combined elimination process.

@item
Add results to a number of databases

@item
Train the databases using the C5.0 machine learner. These
databases will be used in the prediction phase.
@end itemize

We now describe each of these steps in more detail.

@subsubheading Compiling Mantevo benchmarks using combined elimination

This first step involves taking a subset of the Mantevo
benchmarks, and using the combined elimination process to
search for a set of flags which produces lower energy
consumption than -O3.

The set of benchmarks used during this step are as follows:

@itemize
@item CloverLeaf
@item CloverLeaf3D
@item CoMD
@item MiniGhost
@item MiniMD
@item Pathfinder
@item TeaLeaf
@end itemize

@c Feature extractor plguin
@c GCC wrapper

Combined elimination is run for each of these benchmarks
independently, and the intermediate results holding
features and compilations are stored in a separate database
for each benchmark. In the end the database for each
benchmark will contain the features for that benchmark,
along with the record of each compilation that was made
during the combined elimination process. 

Before the combined elimination process is started for a
benchmark, a few extra builds are performed.

@itemize
@item A feature extraction build - Each benchmark is
compiled once using the GCC feature extract plugin to
generate features. These extracted features can then
be reused by all subsequent compilations of the same
source files.

@item A coverage build - Energy results from Allinea MAP
give the total energy used in a function, however we
are interested in the average energy used in each
invocation of a function. This means the total
energy for the function needs to be divided by the
trip count for the function, which can be extracted from
a coverage build. The coverage build is provided the
same input as the benchmark runs so that the coverage
data is accurate.

@item Comparison runs at -O3 and -Os - The benchmark
is built and run at -O3 and -Os in order to have a
point of comparison to compare the results from the
combined elimination runs.
@end itemize

Each test run generated by combined elimination is
run single-threaded on a single node of the NEALE
machine. This allows multiple test runs and multiple
benchmark runs to be done in parallel without interfering
with each other substantially, and speeds up the data
gathering process substantially. Only some stages of
the combined elimination process can be parallelized,
but running in parallel where possible cuts down the
time to gather results considerably.

Each of the benchmarks being run is given inputs which
cause the program to run for around 5 minutes. This is
enough time for the profile to gather a decent number of
samples to minimize error, whilst avoiding slowing down
the combined elimination process too much.

The end result of this stage is as follows:

@itemize
@item A file containing identifiers for the features
for each program unit for the benchmarks.

@item Coverage files, giving the trip counts of each
of the functions when run with the input data.

@item Multiple builds of each benchmark under the
configurations searched by the combined elimination
process, as well as files containing all of the
identifiers for each of the compilations performed
during the combined elimination process.

@item Allinea MAP profiles for each of the executables
generated and run during the search.
@end itemize

From the energy profiles from these executables results
can be derived to be associated with the compilations.

@c Allinea MAP
@c Databases
@c Feature extraction run
@c Coverage run
@c Each benchmark iteratively compiled separately
@c This process done in parallel

@subsubheading Adding results to databases

The combined elimination runs generated a huge number
of energy profiles from which results can be derived
to be associated with the compilations.

The aim of the training phase is to associate the
features of program units and the set of flags used
to compile the program unit, with the resultant
enegy measurement for that program unit.

THe program units which were under compilation
were the functions and modules which appeared in the
source for each benchmark. Therefore, a meaningful
energy measurement value must be extracted from the
profile data to be associated with each program unit.

The profile data output by MAP gives a measurement
of the total energy spent in a function.  This does not
take into account the number of times the function was
executed, so a function which was only executed once
will appear to have a much lower energy measurement
than a function with similar features which was
executed a hundred times. The energy measurement for
each function must therefore be normalized by the
number of times the function was called. This will then
give a measurement of the `quality' of a compilation
of the function which factors out the number of calls
to that function.

To achieve this, we use the coverage data extracted
before combined elimination was performed on the
benchmark. The coverage run used the same input
data as the runs done during combined elimination,
so it gives us an accurate view of the trip count
of each function. By dividing the total energy for
each function by the trip count, we can effectively
normalize the energy data.

The energy used by a module is harder to quantify. We
don't have a measure of the `energy' used by a module,
because the energy for the module is distributed among
the functions which it is comprised of. Therefore
in order to get a measure of the energy usage of a
module we use the average of the previously normalized
energy measurements of the functions which comprise the
module.

The outcomes of this process is that we have energy
values associated with each compilation of a program
unit. Where the compilation identifiers the features
of the program unit and the flags used to compile it.

For a function program unit, the features and
flags used to compile that function are associated
with the average amount of energy used each time
the function is called.

For a module program unit, the features and flags
used to compile that module are associated with the
average amount of energy used in calls to any of
the functions in the module.

After deriving these results they can be added
to the database for each of the respective
benchmarks. For example, the Cloverleaf database
will then contain; the features derived from the
Cloverleaf source; the parameters representing the
flags used to compile program units; records of each
compilation performed during combined elimination,
which associate features with parameters; results
associated energy measurements with each of the
compilations (and by extension the features and
parameters used in that compilation).

At this stage, each benchmark has been kept in
isolation. So the database for Cloverleaf contains
only features, parameters, compilations and results
for Cloverleaf and nothing else. The next stage of
the process is to train the databases, however as
it stands if we trained the Cloverleaf database we
would end up training for just one benchmark.

In order to get useful results, we want to train
the machine learner against data derived from
multiple benchmarks. This is achived by merging
multiple databases to produce larger datasets to
train against.

To maximize the effectiveness of the database,
it is best to include all of the data from all
of the benchmarks. However for the purpose 
of experimentation it is also useful to run
some tests against the input training set.

In the end, eight seperate aggregated databases
were created. One containing results for all of
the benchmarks, and then seven containing
results for all of the benchmarks except one. This
databases are then ready to be trained.

@subsubheading Training using C5.0

During training, the eight databases which were
constructed were each trained using the C5.0 machine
learner in turn.

The training process creates a separate training
blob for function program units and module program
units. For C5.0, each blob is a set of classifier
trees, one for each parameter, where the branches
in the classifier tree are predicated on the values
of features.

For the experiment, the GCC wrapper is being used.
This wrapper alters the flags used to compile
individual source files, and therefore does not
have the ability to make decisions on a
function-by-function basis. Therefore, although
the framework in theory has the ability to make
decisions at the function-level, in this experiment
only module (source file) level decisions are done.

One the training is complete, the training blob
is stored back in the database. This trained
blob will be retrieved when the machine learner is
used during the prediction phase.

@subsubsection Prediction phase

After the various databases were trained, they were
used to try and predict good optimizations for two
different sets of programs; a subset of the Mantevo
benchmarks, including benchmarks included in the
training set as well as benchmarks not previously
seen; and previously unseen programs, which formed a test
of MAGEEC against real-world codes which need
to be optimized.

@subsubheading Testing against Mantevo benchmarks

The databases used the following Mantevo benchmarks
as sources of training data:

@itemize
@item CloverLeaf
@item CloverLeaf3D
@item CoMD
@item MiniGhost
@item MiniMD
@item Pathfinder
@item TeaLeaf
@end itemize

From these training sets, various trained databases
were derived. A database trained against against
all of the benchmarks, and seven databases trained
against all of the benchmarks but excluding a single
benchmark from each database in turn.

Each of these databases was then used to try and
optimize another set of the Mantevo benchmarks, which
included the following programs:

@itemize
@item CloverLeaf
@item CloverLeaf3D
@item CoMD
@item MiniAMR
@item MiniFE
@item MiniGhost
@item MiniMD
@item Pathfinder
@item TeaLeaf
@end itemize

This set is the same as the training set, with the
addition of the MiniAMR and MiniFE programs.

All of the databases were used to optimize all of
the above Mantevo programs, so in many cases
a program was optimized which had already been
encountered in the training data. This means
that the results from these optimizations tests
are a mixture of previously-unseen and
previously-seen programs. The results will be analyzed
in more detail in the results section.

@subsubheading Blind test against new benchmarks

As well as running tests against Mantevo benchmarks,
a couple of tests were performed optimizing new
benchmarks for real-world code. This served as a
check of the effectiveness of MAGEEC for real
programs.

The programs used for this were as follows: ...

@c TODO


@c There are two phases to the experiment; a training phase
@c where a subset of the Mantevo benchmarks are optimized
@c using the combined elimination process and then used
@c to train a database; and a optimization p

@c For the experiment we use a subset of the Mantevo benchmark
@c programs. There are two phases to the experiment, training
@c using a set of benchmarks, and optimizing 
@c training, and optimizing
@c For the experiment
@c We used a subset of the Mantevo programs during the training:

@c * Use the following benchmarks from Mantevo set ...
@c   (Use inputs tuned to take about 5 minutes single execution)
@c * Run feature extractor on each benchmark to get features
@c * Run an coverage run of each benchmark to get trip counts for
@c   each function. This will be used later in order to determine
@c   the amount of energy used during each function invocation.
@c * Run combined elimination to do a guided search for each benchmark
@c   Build a MAGEEC database for each benchmark.
@c * Use GCC wrapper to record each compilation, associated with features
@c   extracted in previous step
@c * Run each benchmark single threaded (no OpenMP or MPI) on NEALE.
@c   Use available nodes to run multiple benchmarks in parallel
@c * Use Allinea MAP to extract energy data for each benchmark run
@c   This will give us total energy usage of the program (used to
@c   drive combined elimination), and energy usage per function
@c * For every energy measurement taken during the benchmark runs.
@c   Derive per-function and per-module energy measures. Take the
@c   total function energy from Allinea MAP, and divide by the
@c   trip counts for each function in the benchmark from the coverage run.
@c   Use the average function energy as the energy measure for the module.
@c diagram or better explanation of this process
@c * For each benchmark, add the results to its database
@c * Make multiple merged databases ...
@c * Trained each database using the C5.0 machine learner

@c NEALE
@c Combined elimination
@c Mantevo
@c Allinea MAP to measure energy
@c Trained for ...
@c Optimized for ...

@subsection Results

We now look at the results from the experimental runs performed.
First we look at the results from the tests against the benchmarks
in the Mantevo set, and then we look at the results from the
blind tests of a new HPC code.

@subsubsection Mantevo benchmark results

Below is a table of the results derived from the Mantevo
benchmarks, when predicting good optimizations using MAGEEC
(through the GCC wrapper driver) along with the various
trained databases.

@center @image{"results-table", 150mm}

There is a separate row for each benchmark which was
optimized, and a separate column for each of the
runs being compared. The meanings of the headers of
the columns is as follows:

@itemize
@item O3 - A run of the benchmark at O3

@item Os - A run of the benchmark at Os

@item All Flags - A run of the benchmark where all available
optimization flags were enabled

@item Combined Elimination (best) - The best run of the
benchmark found in the combined elimination process.

@item All benchmarks trained - Benchmarks optimized using
the database holding the machine learner trained for all
of the benchmarks in the trained set.

@item No CloverLeaf3D - Benchmark optimized used a
database holding a machine learner trained against all
benchmarks except CloverLeaf3D.

@item No CloverLeaf - Benchmark optimized used a
database holding a machine learner trained against all
benchmarks except CloverLeaf.

@item No CoMD - Benchmark optimized used a
database holding a machine learner trained against all
benchmarks except CoMD.

@item No MiniGhost - Benchmark optimized used a
database holding a machine learner trained against all
benchmarks except MiniGhost.

@item No MiniMD - Benchmark optimized used a
database holding a machine learner trained against all
benchmarks except MiniMD.

@item No PathFinder - Benchmark optimized used a
database holding a machine learner trained against all
benchmarks except PathFinder.

@item No TeaLeaf - Benchmark optimized used a
database holding a machine learner trained against all
benchmarks except TeaLeaf.
@end itemize

The values in each cell are the energy usage of the
run as a percentage of the O3 energy usage.

The blue highlighted entries are the important entries.
Those entries are the ones where the benchmark being
optimized was not present in the set used to train
the machine learner. They therefore represent how well
the machine learner performs when it encountered a
previously unseen benchmark.

Note that MiniAMR and MiniFE never appeared in the
training sets, so all of their values are highlighted.
Also note a couple of entries where the value is `0',
signifying no data was derived at that point.

@c Bar graph of results relative to -O3, benchmarks along bottom, different 
@center @image{"results-bar-graph-1", 150mm}
@c filename[, width[, height[, alttext[, extension]]]]}

@center @image{"results-bar-graph-2", 150mm}
@c filename[, width[, height[, alttext[, extension]]]]}

@c Improvements over -O3
@c Number of configurations of flags used
@c Feature diversity
@c Benchmark diversity

@section Conclusion

@c TODO

@section Future work

@c TODO


@c Features
@c Machine learners
@c Optimizing for size
@c Optimizing for energy
@c NEALE

@c Cross-platform (in theory)
@c Flexible to different compilers (GCC implemented)
@c Flexible to different languages (C, C++, Fortran implemented)
@c Integrated or standalone (closed source compilers)
@c Library design
@c Based on flags or compiler internal decisions
@c Open source
@c Plugin interface for different machine learners

@c Justifications for decisions
@c Important terms
@c Combined elimination
@c Experiments
@c Results
@c Limitations
@c Future work
@c Cross platform
@c Multi-compiler
@c Based on flags or other
@c Multi-language (gfortran, gcc, g++)
@c NEALE/Hartree systems
@c Modules/Libraries
@c Gathering
@c Adding results + training
@c Workflow
@c Optimizing
@c Benchmarks (mantevo)
@c Feature quality
@c Diverse feature sets
@c Tools (gcc driver, standalone tool, feature extractor, scripts)
@c Documentation of tools and their flags
@c Considerations for benchmarking on supercomputers
@c Terms (Features/Parameters/Compilation/Program unit)
@c SQLite as an intermediate format
@c Minimize intermediate files
@c Structured, easy to debug (sqlitebrowser, sqlite3)
@c Simple conceptual model
@c Multiprocess access and good performance
@c Allinea MAP
@c Machine learners
@c C5.0
@c 1NN
@c Design decisions
@c Open source

@node References
@chapter References

@c Just using Harvard style references for the moment

@multitable @columnfractions .05 .95

@item [1] @tab Using the GNU Compiler Collection (GCC), (2017).
@i{Instrumentation Options}. [online] Available at: https://gcc.gnu.org/onlinedocs/gcc/Instrumentation-Options.html [Accessed 31 May. 2017]

@item [2] @tab Fursin, G., Miranda, C., Temam, O., Namolaru, M.,
Yom-Tov, E., Zaks, A., Mendelson, B., Bonilla, E., Thomson, J.,
Leather, H. and Williams, C., 2008, June.
MILEPOST GCC: machine learning based research compiler. In @i{GCC Summit}.

@item [3] @tab DB Browser for SQLite, (2017). @i{DB Browser for SQLite}.
[online] Available at: http://sqlitebrowser.org/ [Accessed 31 May. 2017]

@item [4] @tab Pan, Z. and Eigenmann, R., 2006, March. Fast and
effective orchestration of compiler optimizations for automatic
performance tuning. In @i{Code Generation and Optimization, 2006.
CGO 2006. International Symposium on} (pp. 12-pp). IEEE.

@item [5] @tab Pallister, J., Hollis, S.J. and Bennett, J., 2015.
Identifying compiler options to minimize energy consumption for
embedded platforms. @i{The Computer Journal, 58}(1), pp.95-109.

@item [6] @tab Mantevo.org, (2017). @i{Home of the Mantevo Project}.
[online] Available at: https://mantevo.org/ [Accessed 31 May. 2017]

@item [7] @tab Hartree Centre, Science and Technology Facilities Council,
(2017). @i{Resources Available}. [online] Available at:
http://community.hartree.stfc.ac.uk/wiki/site/admin/resources.html
[Accessed 31 May. 2017]
@end multitable

@node Glossary
@chapter Glossary

@menu
* GCC plugin features::
@end menu

@node GCC plugin features
@section GCC plugin features

@subsection Function features

@include function-features.texi

@subsection Module features

@include module-features.texi

@node GNU Free Documentation License
@chapter GNU Free Documentation License
@include fdl-1.3.texi

@bye
