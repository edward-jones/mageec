\input texinfo
@setfilename mageec-design.info
@afourpaper
@settitle MAGEEC Design
@paragraphindent 0

@copying
This design document is for MAGEEC.
This manual is for MAGEEC.

Copyright @copyright{} 2015 Embecosm Limited.
@quotation
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, with no Front-Cover Texts, and with no Back-Cover
Texts. A copy of the license is included in the section entitled
``GNU Free Documentation License''.
@end quotation
@end copying

@titlepage
@title MAGEEC Design
@subtitle MAchine Guided Energy Efficient Compilation
@author Edward Jones
@page
@vskip 0pt plus 1filll
@insertcopying
@end titlepage

@contents

@node Top
@top Scope of this Document
The document outlines the design of MAGEEC @emph{MAchine Guided Energy
Efficient Compilation}.

@menu
* Overview::
* Current optimization approaches::
* Terminology::
* Theory for how MAGEEC works::
* High level features of MAGEEC::
* Comparison with MAGEEC::
* MAGEEC components::
* Feature extraction::
* Interface::
* GCC plugin::
* Compiler driver::
* Driver interface::
* GCC wrapper driver::
* MAGEEC file format::
* Database structure::
* Machine learner interface::
* C5.0 Machine learner::
* 1-NN Machine learner::
* Tooling::
* Standalone MAGEEC tool::
* Feature extraction script::
* Combined elimination script::
* MAGEEC library::
* Workflow::
* Gathering and training::
* Optimizing::
* Experimentation::
* GNU Free Documentation License::
@end menu

@node Overview
@section Overview

MAGEEC is a set of components to augment a compiler with the ability
to make intelligent predictions about which optimizations to run based
on a machine learner trained against statically extracted features of
the program under compilation.

Contemporary compilers mostly use a fixed pipeline of passes when
optimizing. This pipeline can often be configured by the user, either
at a high level to specify the overall degree of optimization desired,
or at a low level to enable or disable specific optimizations, however
for a given invocation of the compiler the pipeline is fixed. The
pass pipelines for common optimization levels are balanced to work
well with the widest range of code which may pass through the
compiler, however for any given piece of code it will not necessarily
be optimal. In many compilers the user may tune the pipeline for a
given program or source file, however this is a non-trivial manual
task which requires a lot of trial and error, and a lot of
iterations of compilation.

@c @image PICTURE OF TYPICAL APPROACH TAKEN WHEN COMPILING

From the perspective of the user, it would be reassuring to know
that the compiler is not missing optimization opportunities. If the
compiler could be relied on to make more precise optimization decisions
it could also be possible to simplify the user experience by
not requiring them to manually disable and enable flags to
squeeze maximum performance from their code.

From the perspective of the compiler writer, it would be nice to
avoid having to manually tune the pass pipeline for each new
processor and CPU. For a given architecture, there can exist a vast
array of CPUs which vary in the hardware features supported and
the intricacies of their microarchitecture. (Example of ARM?)
Tuning the compiler for each of these variants is laborious. 

A further point of interest is that most compilers currently
optimize for speed, and give less attention to code size, energy,
or any other interesting metrics. In general optimization can only
be done with one goal (speed/code size) and it is not possible to
express more complex optimizations goals such as optimizing for
energy and code size at the same time, or optimizing for energy
after a code size target is achieved.

If selecting the best compiler configuration can be automated,
then there are several potential benefits which can be realized.
  
@itemize
@item
Remove complexity from the compiler to tune optimization
pipelines for different ISAs, microarchitectures, CPU features
sets, and metrics.

@item
Provide the user a more straightforward interface, as well
as better performance of resultant programs.

@item
Control optimizations at a finer granularity (file, module) to
realize performance improvements which cannot be realized in current
compilers (which can only control the pass pipeline at a
per-invocation level).

@item
Optimize for new metrics such as energy or compilation speed.

@item
Provide a more powerful interface to control tradeoffs
between different metrics, such as balancing energy and size.
@end itemize

@c @image DIAGRAM OF MORE IDEAL OPTIMIZATION APPROACHES (module + function level)

@node Current optimization approaches
@subsection Current optimization approaches

As it stands, most compilers appear to use a fixed pipeline of
passes when optimizing. However there has been other work to make
pass control more powerful. Some of these techniques exist in
contemporary compilers, however many only appear in research.

@itemize
@item
Profile-guided optimization: This is common in many open source
and propreitary compilers now. Optimization is tuned based on
profiling data from running the program with representative
data. The profile provides information about the hot-spots in
the code, which can be used to inform the ordering of basic
blocks and the ferocity of inlining.

@c @item
@c MILEPOST

@c @item
@c MAGEEC project

@c @item
@c Citations for other authors' work
@end itemize

@node Terminology
@section Terminology

@itemize
@item
Program Unit - A contiguous part of a program which can be uniquely
identifier in both the original source code and the final executable.
Examples are modules, functions and possibly loops and basic blocks.

@item
Feature - A measurable and quantifiable aspect of a program unit, which
represents an aspect of the program unit in some way. For example,
the number of instructions in a basic block.

@item
Parameter - A value which can be controlled by some mechanism in the
compiler to affect the decree or nature of optimization, with the
effect of changing the object code produced by the program. For example:
a flag indicating whether the 'dead code elimination' pass should be
executed.

@item
Compiler configuration - A set of parameters used to compile an
individual program unit. Multiple units of the same program may
use the same configuration of the compiler.
@end itemize

@node Theory for how MAGEEC works
@section Theory for how MAGEEC works

MAGEEC automates the selection of optimizations by using a
machine learner to may key decisions about the pass to
run and any tunable parameters which may be set. An
association is learned between features derived from the
input source code, and the appropriate parameters to use in
the compilation to build the executable. Like all machine
learning processes, there is a training stage; where the
machine learner is fed features and corresponding 'good'
parameters to build up an association; and a optimizing stage;
where the machine learner is used with a new, previously
unencountered program and used to select a set of parameters
which are likely to produce a good quality output executable.

@c @image High level diagram of the process (with MAGEEC-instrumented compiler)

@node High level features of MAGEEC
@section High level features of MAGEEC

MAGEEC using static features, which are derived ahead of time from the
input program. This has some advantages and disadvantages compared to
other approaches.

Advantages:
@itemize
@item
Test programs do not have to be recompiled to receive a benefit. Interactive
compilation techniques such as combined elimination require a large number
of recompilation runs, and even just doing a single profiled run to
derived dynamic features can be prohibitively difficult if the program is
large or cannot easily be instrumented.

@item
Measure static features is simpler than measuring dynamic features. Dynamic
features may not be precisely measureable, may depend on the environment,
and require a more complex build stage.

@item
Measuring static features is quicker. The effect on compilation time can
be minimized with static features, as they can be very easily derived from
the source code or intermediate representation. This makes deployment
far easier.

@item
In some cases it may not be possible to instrument and measure dynamic
features. For example, an embedded system may not have enough memory
for the overhead of instrumentation, or it may not be possible to
measure some features on an embedded systems
@c Example (limitations in debug interface)
@end itemize

Disadvantages:
@itemize
@item
Static features cannot express the effects of the input data on the
program. Two program could have identical features, but exhibit very
different performance characteristics depending on the data provided
to it. Indeed, one program can also exhibit very different performance
depending on the data provided to it. This can be mitigated, as the
features of the program will often hint at the type of data used with
it, however the problem cannot be mitigated.

@item
Static features are less representative of the input program. This
relates to the previous point. Dynamic features are likely to be
more representative of the input program than static features. This
means that static features are likely to be far less effective than
using dynamic features. It also means more training data is likely
to be needed.

@item
If static features are derived too easier, they may be skewed by the
structure of the input code. @c Not really a problem
@end itemize

MAGEEC is open source under the GPL license. The aim is that by doing
this MAGEEC can be freely implemented in as wide range of compilers as
possible.

The design of MAGEEC makes it possible to use a variety of machine
learners through a common interface, and makes it possible to use
both command-line driven proprietary compilers, and open source
compilers. The interface to MAGEEC can either be a wrapper around
the compiler (ideal for a proprietary compiler), or can integrate
in the compiler itself, possibly through a plugin interface. Deeper
integration beneficial as it may allow compilation controlling
decision to be made at a finer granularity than program or module
level.

@node Comparison with MAGEEC
@section Comparison with MAGEEC

The original MAGEEC was aimed at optimizing for energy on embedded
systems. The aim of this updated version was to also target high
performance application and also to improve the workflow to make
the optimization process more robust.

@node MAGEEC components
@section MAGEEC components

@c @image High level diagram of gather+train with compiler, possibly later? (workflow?)

MAGEEC itself is not a machine learner, and is instead a
set of components and utilities which allow a compiler to
be intrumented with some 'machine-learning' capabilities. This
includes:

@itemize
@item
An interface to derive static features from an input program
(GCC feature extraction)

@item
A mechanism to drive the compiler to exercise various
parameters during the gather phase. (GCC wrapper)

@item
A file format to record features, compilations, and results
in a centralized place. (SQLite)

@item
An interface to a number of machine learners, and the
machine learners themselves.

@item
Tools and scripts to access the file, and drive the training
and optimization process.
@end itemize

@c @image High level diagram of MAGEEC components
@c @image Design of MAGEEC in general

We now describe each of these components in more detail, and
their current implementations.

@node Feature extraction
@subsection Feature extraction
@node Interface
@subsubsection Interface

Feature extraction involves taking an input source program, and
outputting a set of features for each of the individual
program units in the program. Each source file can produce
multiple sets of features for each different program unit in 
the file. So the module will produce a feature set, and each
function could produce its own feature set. It is even feasible
that feature could be generated for each basic block or loop.

MAGEEC provides an interface to create a set of features. How
exactly the features are extracted is up to the program doing the
feature extraction, and the features which are extracted is also
at the whim of the feature extractor. The limitations placed by
MAGEEC are that the type of the features must be one of a set of
known types (this set of types is common with the types which
the machine learners can handle), and a given extracted feature
is consistently identified by the name numerical identifier.

The interface to MAGEEC allows a feature extractor to insert a
new set of features into the file for a program unit, and
receive a unique identifier for those features back.

@node GCC plugin
@subsubsection GCC plugin

As well as the interface to the feature extractor, a feature
extractor plugin in also provided for the GCC compiler. This
plugin can be provided as a plugin to gcc, and used to
extract module and function features from the program.

This plugin takes the input program and outputs a .csv file
identifing an individual program unit in the source code, associating
it with the unique identifier of the feature set for that
program unit.

The benefit of the GCC feature extractor is that it allows feature
extraction to be performed on C, C++ and Fortran code without
requiring a compiler-specific feature extractor to be written.
Provided code can be built with GCC, feature extraction can be
achieved. This makes it ideal for instrumenting propreitary
compilers.

The features extracted by the GCC feature extractor are as follows:

@c List of features
@c Description of features
@c @image High level diagram of feature extraction process
@c @image Diagram of feature extractor plugin process

@node Compiler driver
@subsection Compiler driver

In order to drive the training and optimization process, there needs
to be a mechanism to control the parameters provided to the compiler.
When training, it needs to be possible to run the compiler under a
number of controlled configurations, and record each of these
compilations in the MAGEEC database for later use. During optimization
it needs to be possible to call out to MAGEEC to make decisions using
a trained machine learner and then use these decisions to control
compilation parameters.

@node Driver interface
@subsubsection Driver interface

MAGEEC offers an interface, which allows the compiler, or a wrapper
controlling the compiler to store a record of each program unit
being compiled, the features associated with that program unit, and
parameters describing the configuration of the compiler for that
compilation. The interface takes all of the information describing
the compilation of a program unit; the features and parameters
involved in the compilation, and returns a unique identifier for
that compilation. Later, results will be associated with the
compilation, and then there will be enough information to train
a machine learner.

For optimization, an interface is also offered which allows the driver
to use a previously trained machine learner to make decisions about
parameters. The driver can query a machine learner with a set of
feature, and receive a decision. It can then use this decision to control
the compilation of a program unit.

@node GCC wrapper driver
@subsection GCC wrapper driver

MAGEEC offers one concrete interface to drive GCC for training and
optimization purposes which takes the form of a GCC wrapper. This
driver is a wrapper around the gcc/gfortran/g++ drivers, which
intecepts and controls the optimization flags passed to it.

When training, the driver tracks the optimization flags being used
with the driver and records a compilation in the database for each
program unit.

When optimizing, the driver strips optimization flags and instead
replaces the flags with flags derived through multiple calls out
to the MAGEEC interface to the machine learner. It uses previously
extracted features to inform this process.

Because the GCC wrapper driver works externally to the compiler, it
does not require any invasive changes to the compiler in order to
control the passes. It also supports multiple version of GCC easily
and can support Fortran and C++ code through the same driver.
Additionally because it uses the public documented interface to the
compiler, it is more stable as it cannot create broken pass pipelines <-- WUT? -->

A disadvantage of the GCC driver is that it is only able to make decisions on
a per-module level. If the driver instead operated internally to GCC it could
be possible to enable or disable passes on a per function basis. This
severely limits the optimization potential.

@c Flags vs internal pass decisions
@c High level diagram of compilation process for gather+optimize
@c @image Diagram of GCC wrapper driver process

@node MAGEEC file format
@subsection MAGEEC file format

Internally MAGEEC uses an SQLite database as an intermediate format.
This MAGEEC database is the main file format, and is used to record
the intermediate stages of training. It holds extracted features,
records of each compilation and the accompanying parameters and features
for it, results, trained machine learners, and various debug information.

When the feature extractor is run, features are added to the database,
and a handle to an entry in the database is returned. Likewise,
when a program unit is compiled with a with a set of parameters,
a compilation is recorded in the database an an identifier to it
is returned. This compilation can then have a result value associated
with it later.

There are a few reasons for using an SQLite database as an
intermediate format:

@itemize
@item
The data by its nature contains relations which make it conducive
to using a database. By using a database, this relations can be
maintained and enforced.

@item
The database is a familiar conceptual model which is easier to
understand.

@item
sqlitebrowser can be used to query the database manually, making
it easier to debug and manually fix if it is ever necessary.

@item
SQLite allows multiple processes to read/write the same file,
which is useful for parallelizing the training process for
performance. Doing this with flat files would be more complex,
and require reimplementing functionality already offered by
SQLite.

@item
SQLite is quite high performance, and offers resilience against
corrupt data out of the box.
@end itemize

The main disadvantage to using SQLite is that it adds some
complexity to the interfaces, and requires more boilerplate code
to interface than a simpler flat file solution would.

@node Database structure
@subsubsection Database structure

Below is a description of the file format structure and the
associated relations.

@c @image Database structure diagram

@node Machine learner interface
@subsection Machine learner interface

The machine learner interface provides the capability to train
a machine learner given a set of compilations and their results,
as well as make decisions using a previously trained machine learner.

When training, MAGEEC provides the machine learner with all of the
results data for all of the compilations which have been performed.
This associates a set of inputs features compiled using a particular
set of parameters to the compiler, with a single output result
value. The interface also provides a set of parameters which the
user may wish to make decisions on, and which therefore forms the set
of parameters which the machine learner should train to make a
decision against.

The machine learner can use the dump of data however it likes.
However, the end result of the training process is a single binary
blob which is opaque to MAGEEC and the MAGEEC file format, but
can be deciphered by the machine learner at a later time when it
is required to make a decision. This binary blob is returned by
the machine learner and then stored by MAGEEC in the database for
use later.

When optimizing, the blob for the machine learner is retrieved
and an instance of the machine learner is instantiated. The
user of the machine learner can then query the machine learner
to make decisions about parameters which the machine learner
encountered when training. The machine learner is provided the
decision to make, as well as a set of features to use in order
to make the decision. The machine learner can choose to make a
decision, or it can refuse, leaving it up to the user to then
make the decision itself.

The features provided to the machine learner are made up of a
unique identifier, as well as a 'type'. The unique identifier
is guaranteed by the feature extractor to unique identify the
same feature every time, and the type is one of a number of
types specified by MAGEEC.

The parameters which are provided to the machine learner also
have a unique identifier which is guaranteed to identify the
same parameter in the compiler which is being used. The
types of the parameters is also an identifier for a type specified
by MAGEEC.

@c Feature types parameter types, decisions?

Currently, there is no ability to do incremental learning.

@c @image Diagram of machine learner inputs and outputs for train + optimize

The machine learning interface is used by the standalone tool
in order to train a database based on generated results, and
it is used by the GCC wrapper driver in order to make decisions
during compilation.

In MAGEEC there currently exist two machine learners, a machine learner
based on the C5.0 classifier, and a machine learner based on a one
nearest-neighbor search. These are built as libraries which the
GCC wrapper then links against.

A user can create a new machine learner, provided it implements the
machine learner interface. It is then possible to use this machine
learner as a plugin to the standalone tool and GCC wrapper, by
building the machine learner as a dynamic library which the
tools can link against.

@node C5.0 Machine learner
@subsubsection C5.0 Machine learner

C5.0 is a machine learning which creates a classifier tree based on
the input features. For MAGEEC, a C5.0 classifier tree is created
for each decision which may be made (corresponding to a parameter
in the compilation). The machine learner is provided the best
result for each distinct set of features, and it uses these
to build a classification tree for each parameter. The classification
tree for each of the parameters are concatenated together and
stored in the blob.

When a decision must be made, the appropriate classifier tree is
found, and the features for the program unit are used to determine
the classification for that decision. For a boolean decision, the
resulting decision could be 'true', 'false', or 'native' if the
machine learner is not able to make a decision.

@c @image Diagram of classifier trees for MAGEEC
@c @image Diagram of use of a classifer tree for MAGEEC

Advantages of C5.0 are that the interface is straightforward
and the decision trees are human readable.

@node 1-NN Machine learner
@subsubsection 1-NN Machine learner

One nearest neighbor is a conceptually very simple classification
method. Each feature set form a point in n-dimensional space, where
each feature is mapped to an individual axis. Each feature set 'point'
is associated with a parameter set, which is the parameter set for
the best result for that feature set in the results data. When a
decision needs to be made, the closest feature set to the input
set is found in the training data, and the decision is made based
on the parameters in that closest point.

1-NN is a very simply classification technique, and is very easy
to understand. 

@c @image Diagram of 1NN classification for MAGEEC

@node Tooling
@subsection Tooling

As well as the key MAGEEC interfaces and components, there are also
a few tasks which are achieved through tooling and python scripts.

@node Standalone MAGEEC tool
@subsubsection Standalone MAGEEC tool

This tool exists to do a few tasks which can't be done by either
the feature extractor or compiler driver. This includes; adding
results into the database, training a database using a machine
learner; and creating and merging databases files. It is a simple
command line tool.

@node Feature extraction script
@subsubsection Feature extraction script

This script exists to build and feature extract a program using the
GCC feature extractor plugin. It simplifies the interface to the
plugin, and automatically handles autoconf and CMake build systems.

@node Combined elimination script
@subsubsection Combined elimination script

This script performs the combined elimination process as described in @c citation.
This script exists in order to do an iterative
search towards the best combination of compilation flags for a
benchmark, which can be used to effectively train many machine learners.
The naive way to find good flag combinations is to do a random
search, however combined elimination finds more optimal results in
far fewer runs. Even compared to doing a fractional factorial design
combined elimination is much more effective.

The combined elimination process starts with all flags enabled,
and then attempts to disable flags in groups in order to find a
more optimal set of flags for a given program. It has been shown
to be more effective than other iteractive compilation techniques.

@c @image Combine elimination diagram

Combined elimination is used by MAGEEC primarily for finding good
data points for the training process.

@node MAGEEC library
@subsection MAGEEC library

The various interfaces exposed by MAGEEC, and used by the GCC
feature extractor plugin, GCC wrapper driver and standalone tool
are all implemented in a core library which the MAGEEC tools,
and other software which wishes to use with MAGEEC can link
against. By including this in a library, it means that integration
with MAGEEC can be embedded into other compilers. This could
be used to perform feature extraction from within another
compiler, or to allow MAGEEC to alter the internal pass
sequences in a compiler.

@node Workflow
@section Workflow

There are two phases to using MAGEEC. Gathering and training a
machine learner, and optimizing a program based on a trained
machine learner.

@node Gathering and training
@subsection Gathering and training

The gathering process involves repeatedly building a number of
benchmarks under slightly different configurations of the
compiler. The resultant binaries can then be measured somehow,
and the results of that measurement can be associated
with the compilations of each of the program units which make
up the executable.

The aim of the gathering process is to find sets of parameters
which minimize the cost function (with the aim to beat the
default configurations of the compiler), and there are a few ways
to find this set.

The ideal approach would be to do an exhaustive search of the
parameter space to find the absolute best set of flags. If
possible, this would be ideal as you can guarantee that the
best result has been found. The problem with this approach
is that it is simply not possible to do an exhaustive search
of the parameter space. For a compiler such as GCC where there
are approximately 100 boolean optimization flags which can be
configured, there are 2^100 combinations which would need to
be tested.

Since an exhaustive search is not feasible, an alternative is
to only search a subset of the full search space. There are
several observation one might be able to make in order to
cut down the search space considerably:

@itemize
@item
In general, configurations with most flags on do better than
configurations with less flags. It is extremely unlikely
that enabling only a single flag would be better than -O3

@item
As a first approximation, most flags can be treated
independently.

@item
Many flags will have very little effect either way, and can
be excluded
@end itemize

Taking factors like this into account, it is possible to
trim down the search space considerably. Using techniques such
as fractional factorial design can be used to only test
a subset of combinations. In some cases, this is viable, however
for many programs used in the training process, compilation
times may be sufficiently high that it is worth looking for
better solutions.

In order to minimize the number of tests, incremental compilation
can be used in order to hill-climb towards better solutions.
There are various techniques that can be used for this, however
combined-elimination is straightforward and has been demonstrated
to find promising results.

Incremental compilation suffers from the risk that only a local
minima is found, however provided this local minima is still
an improvement over the default compiler behavior, this is
a trade-off worth making.

Another major component to the gathering process is the set
of benchmarks which are used. The benchmarks needs to be
representative of the types of programs which will be used
with the compiler. They also need to be varied, so that the
feature sets gather cover as many potential input programs as
possible, and most importantly they need to be numerous.
Machine learning requires a glut of data to produce reliable
results, so having a lot of benchmarks containing a lot of
modules and functions is important.

After the benchmarks have been run under a large number of
configurations, and they have been measured and their results
gathered into MAGEEC, they can be trained. Training involves
invoking the machine learner to produce a training blob
which is stored and used at a later point.

@node Optimizing
@subsection Optimizing

In comparison to gathering and training, the optimizing phase is
much simpler. Here a new program has its features extracted, and
these features are then passed to MAGEEC along with decisions
which need to be made. The decisions about parameters are made,
and this is used to inform the compilation of the program.

The result should be a binary which is more optimal than would
be produced by the default compiler.

@node Experimentation
@section Experimentation

@c Features
@c Machine learners
@c Optimizing for size
@c Optimizing for energy
@c NEALE

@c Cross-platform (in theory)
@c Flexible to different compilers (GCC implemented)
@c Flexible to different languages (C, C++, Fortran implemented)
@c Integrated or standalone (closed source compilers)
@c Library design
@c Based on flags or compiler internal decisions
@c Open source
@c Plugin interface for different machine learners

@c Justifications for decisions
@c Important terms
@c Combined elimination
@c Experiments
@c Results
@c Limitations
@c Future work
@c Cross platform
@c Multi-compiler
@c Based on flags or other
@c Multi-language (gfortran, gcc, g++)
@c NEALE/Hartree systems
@c Modules/Libraries
@c Gathering
@c Adding results + training
@c Workflow
@c Optimizing
@c Benchmarks (mantevo)
@c Feature quality
@c Diverse feature sets
@c Tools (gcc driver, standalone tool, feature extractor, scripts)
@c Documentation of tools and their flags
@c Considerations for benchmarking on supercomputers
@c Terms (Features/Parameters/Compilation/Program unit)
@c SQLite as an intermediate format
@c Minimize intermediate files
@c Structured, easy to debug (sqlitebrowser, sqlite3)
@c Simple conceptual model
@c Multiprocess access and good performance
@c Allinea MAP
@c Machine learners
@c C5.0
@c 1NN
@c Design decisions
@c Open source

@node GNU Free Documentation License
@chapter GNU Free Documentation License
@include fdl-1.3.texi

@bye
