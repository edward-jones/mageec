

Things to write
* Documentation, manual
* Paper
* Talk
* Marketting cruft


= Overview =

MAGEEC is a set of components to augment a compiler with the ability
to make intelligent predictions about which optimizations to run based
on a machine learner trained against statically extracted features of
the program under compilation.

Contemporary compilers mostly use a fixed pipeline of passes when
optimizing. This pipeline can often be configured by the user, either
at a high level to specify the overall degree of optimization desired,
or at a low level to enable or disable specific optimizations, however
for a given invocation of the compiler the pipeline is fixed. The
pass pipelines for common optimization levels are balanced to work
well with the widest range of code which may pass through the
compiler, however for any given piece of code it will not necessarily
be optimal. In many compilers the user may tune the pipeline for a
given program or source file, however this is a non-trivial manual
task which requires a lot of trial and error, and a lot of
iterations of compilation.

<-- Picture of typical approach taken when compiling -->

From the perspective of the user, it would be reassuring to know
that the compiler is not missing optimization opportunities. If the
compiler could be relied on to make more precise optimization decisions
it could also be possible to simplify the user experience by
not requiring them to manually disable and enable flags to
squeeze maximum performance from their code.

From the perspective of the compiler writer, it would be nice to
avoid having to manually tune the pass pipeline for each new
processor and CPU. For a given architecture, there can exist a vast
array of CPUs which vary in the hardware features supported and
the intricacies of their microarchitecture. (Example of ARM?)
Tuning the compiler for each of these variants is laborious. 

A further point of interest is that most compilers currently
optimize for speed, and give less attention to code size, energy,
or any other interesting metrics. In general optimization can only
be done with one goal (speed/code size) and it is not possible to
express more complex optimizations goals such as optimizing for
energy and code size at the same time, or optimizing for energy
after a code size target is achieved.

//As a brief test, we did a
//run of combined elimination optimizing for time and found we could
//achieve a ?% improvement compared to -O3, however when we did the
//same but optimized for energy, we found we could achieve a ?%
//improvement (do this analysis).

If selecting the best compiler configuration can be automated,
then there are several potential benefits which can be realized.
  
* Remove complexity from the compiler to tune optimization
  pipelines for different ISAs, microarchitectures, CPU features
  sets, and metrics.
* Provide the user a more straightforward interface, as well
  as better performance of resultant programs.
* Control optimizations at a finer granularity (file, module) to
  realize performance improvements which cannot be realized in current
  compilers (which can only control the pass pipeline at a
  per-invocation level).
* Optimize for new metrics such as energy or compilation speed.
* Provide a more powerful interface to control tradeoffs
  between different metrics, such as balancing energy and size.

<-- Diagram of more ideal optimization approaches (module and function level) -->

= Current optimization approach =

As it stands, most compilers appear to use a fixed pipeline of
passes when optimizing. However there has been other work to make
pass control more powerful. Some of these techniques exist in
contemporary compilers, however many only appear in research.

* Profile-guided optimization: This is common in many open source
  and propreitary compilers now. Optimization is tuned based on
  profiling data from running the program with representative
  data. The profile provides information about the hot-spots in
  the code, which can be used to inform the ordering of basic
  blocks and the ferocity of inlining.
* <-- MILEPOST -->
* <-- MAGEEC -->
* <-- Citations to other authors work -->

= Terminology =

* Program Unit - A contiguous part of a program which can be uniquely
  identifier in both the original source code and the final executable.
  Examples are modules, functions and possibly loops and basic blocks.
* Feature - A measurable and quantifiable aspect of a program unit, which
  represents an aspect of the program unit in some way. For example,
  the number of instructions in a basic block.
* Parameter - A value which can be controlled by some mechanism in the
  compiler to affect the decree or nature of optimization, with the
  effect of changing the object code produced by the program. For example:
  a flag indicating whether the 'dead code elimination' pass should be
  executed.
* Compiler configuration - A set of parameters used to compile an
  individual program unit. Multiple units of the same program may
  use the same configuration of the compiler.

= Theory for how MAGEEC works =

MAGEEC automates the selection of optimizations by using a
machine learner to may key decisions about the pass to
run and any tunable parameters which may be set. An
association is learned between features derived from the
input source code, and the appropriate parameters to use in
the compilation to build the executable. Like all machine
learning processes, there is a training stage; where the
machine learner is fed features and corresponding 'good'
parameters to build up an association; and a optimizing stage;
where the machine learner is used with a new, previously
unencountered program and used to select a set of parameters
which are likely to produce a good quality output executable.

<-- High level diagram of the process (with MAGEEC-instrumented compiler) -->

= High level features of MAGEEC =

MAGEEC using static features, which are derived ahead of time from the
input program. This has some advantages and disadvantages compared to
other approaches.

Advantages:
* Test programs do not have to be recompiled to receive a benefit. Interactive
  compilation techniques such as combined elimination require a large number
  of recompilation runs, and even just doing a single profiled run to
  derived dynamic features can be prohibitively difficult if the program is
  large or cannot easily be instrumented.
* Measure static features is simpler than measuring dynamic features. Dynamic
  features may not be precisely measureable, may depend on the environment,
  and require a more complex build stage.
* Measuring static features is quicker. The effect on compilation time can
  be minimized with static features, as they can be very easily derived from
  the source code or intermediate representation. This makes deployment
  far easier.
* In some cases it may not be possible to instrument and measure dynamic
  features. For example, an embedded system may not have enough memory
  for the overhead of instrumentation, or it may not be possible to
  measure some features on an embedded systems
  <-- Example (limitations in debug interface) -->

Disadvantages:
* Static features cannot express the effects of the input data on the
  program. Two program could have identical features, but exhibit very
  different performance characteristics depending on the data provided
  to it. Indeed, one program can also exhibit very different performance
  depending on the data provided to it. This can be mitigated, as the
  features of the program will often hint at the type of data used with
  it, however the problem cannot be mitigated.
* Static features are less representative of the input program. This
  relates to the previous point. Dynamic features are likely to be
  more representative of the input program than static features. This
  means that static features are likely to be far less effective than
  using dynamic features. It also means more training data is likely
  to be needed.
* If static features are derived too easier, they may be skewed by the
  structure of the input code. <-- Not really a problem -->

MAGEEC is open source under the GPL license. The aim is that by doing
this MAGEEC can be freely implemented in as wide range of compilers as
possible.

The design of MAGEEC makes it possible to use a variety of machine
learners through a common interface, and makes it possible to use
both command-line driven proprietary compilers, and open source
compilers. The interface to MAGEEC can either be a wrapper around
the compiler (ideal for a proprietary compiler), or can integrate
in the compiler itself, possibly through a plugin interface. Deeper
integration beneficial as it may allow compilation controlling
decision to be made at a finer granularity than program or module
level.

== Comparison with MAGEEC ==

The original MAGEEC was aimed at optimizing for energy on embedded
systems. The aim of this updated version was to also target high
performance application and also to improve the workflow to make
the optimization process more robust.

= MAGEEC components =

<-- High level diagram of gather+train with compiler, possibly later? (workflow?) -->

MAGEEC itself is not a machine learner, and is instead a
set of components and utilities which allow a compiler to
be intrumented with some 'machine-learning' capabilities. This
includes:

* An interface to derive static features from an input program
  (GCC feature extraction)
* A mechanism to drive the compiler to exercise various
  parameters during the gather phase. (GCC wrapper)
* A file format to record features, compilations, and results
  in a centralized place. (SQLite)
* An interface to a number of machine learners, and the
  machine learners themselves.
* Tools and scripts to access the file, and drive the training
  and optimization process.

<-- High level diagram of MAGEEC components -->
<-- Design of MAGEEC in general -->

We now describe each of these components in more detail, and
their current implementations.

== Feature extraction ==

=== Interface ===

Feature extraction involves taking an input source program, and
outputting a set of features for each of the individual
program units in the program. Each source file can produce
multiple sets of features for each different program unit in 
the file. So the module will produce a feature set, and each
function could produce its own feature set. It is even feasible
that feature could be generated for each basic block or loop.

MAGEEC provides an interface to create a set of features. How
exactly the features are extracted is up to the program doing the
feature extraction, and the features which are extracted is also
at the whim of the feature extractor. The limitations placed by
MAGEEC are that the type of the features must be one of a set of
known types (this set of types is common with the types which
the machine learners can handle), and a given extracted feature
is consistently identified by the name numerical identifier.

The interface to MAGEEC allows a feature extractor to insert a
new set of features into the file for a program unit, and
receive a unique identifier for those features back.

=== GCC plugin ===

As well as the interface to the feature extractor, a feature
extractor plugin in also provided for the GCC compiler. This
plugin can be provided as a plugin to gcc, and used to
extract module and function features from the program.

This plugin takes the input program and outputs a .csv file
identifing an individual program unit in the source code, associating
it with the unique identifier of the feature set for that
program unit.

The benefit of the GCC feature extractor is that it allows feature
extraction to be performed on C, C++ and Fortran code without
requiring a compiler-specific feature extractor to be written.
Provided code can be built with GCC, feature extraction can be
achieved. This makes it ideal for instrumenting propreitary
compilers.

The features extracted by the GCC feature extractor are as follows:

<-- List of features -->
<-- Description of features -->

<-- High level Diagram of feature extraction process -->
<-- Diagram of feature extraction plugin process -->

== Compiler driver ==

Interface + GCC wrapper

== File format ==

SQLite database

* Feature extraction (interface + gcc plugin)
* Compiler driver (interface + gcc wrapper)
* File format (SQLite database)
* Machine learners (interface + C5.0 + 1NN)
* Ma

<-- Workflow -->

* Gather
* Training

<-- Experimentation -->

* Features
* Machine learners
* Optimizing for size
* Optimizing for energy
* NEALE






Unlike other work in this area, the extracted features are
static features derived from the input source code, rather
than dynamic features derived from an execution of a
program. This has the advantage that features can 

are made based on static features 

MAGEEC automates the selection of optimizations by using a
machine learner to make key decisions about the optimizations
to run, based on a number of statically derived features of
the program. It is designed to either integrate with the
compiler, or be included in a wrapper around the compiler
driver.

WORKFLOW

= Comparison with existing techniques = 

* PGO
* Static vs dynamic features
* Production ready (TM)

= MAGEEC features =

* Cross-platform (in theory)
* Flexible to different compilers (GCC implemented)
* Flexible to different languages (C, C++, Fortran implemented)
* Integrated or standalone (closed source compilers)
* Library design
* Based on flags or compiler internal decisions
* Open source
* Plugin interface for different machine learners

= MAGEEC design =

There are a few components to the whole system:

HIGH LEVEL COMPONENTS

* A feature extractor
  This takes the input source file and extracts interesting
  features which quantity each of the program units which
  comprise the file. The feature extractor can be integrated
  in the compiler, or part of a standalone tool.
* A compiler stub or wrapper
  When gathering, this is used to record the parameters
  which make up the compiler configuation
  When optimizing, this uses extracted features to
  query the machine learner to make decisions about
  the value which compilation parameters should take.

MAGEEC DESIGN

MAGEEC is based very strongly around its file format which is
an SQLite database. The database holds intermediate data during
gathering 

MAGEEC COMPONENTS

FEATURES




MAGEEC is based around a 
MAGEEC itself is made up of a few components, which are as
follows:

* The core MAGEEC library
  This library provides an interface to the majority of
  the functionality. It is used by the feature extractor
  MAGEEC database. 
* A standalone feature extractor


* Important terms
* Combined elimination
* Experiments
* Results
* Limitations
* Future work
* Cross platform
* Multi-compiler
* Based on flags or other
* Multi-language (gfortran, gcc, g++)
* NEALE/Hartree systems
* Modules/Libraries
* Gathering
* Adding results + training
* Workflow
* Optimizing
* Benchmarks (mantevo)
* Feature quality
* Diverse feature sets
* Tools (gcc driver, standalone tool, feature extractor, scripts)
  * Documentation of tools and their flags
* Considerations for benchmarking on supercomputers
* Terms (Features/Parameters/Compilation/Program unit)
* SQLite as an intermediate format
  * Minimize intermediate files
  * Structured, easy to debug (sqlitebrowser, sqlite3)
  * Simple conceptual model
  * Multiprocess access and good performance
* Allinea MAP
* Machine learners
  * C5.0
  * 1NN
* Design decisions
* Open source
