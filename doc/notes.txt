

Things to write
* Documentation, manual
* Paper
* Talk
* Marketting cruft


= Overview =

MAGEEC is a set of components to augment a compiler with the ability
to make intelligent predictions about which optimizations to run based
on a machine learner trained against statically extracted features of
the program under compilation.

Contemporary compilers mostly use a fixed pipeline of passes when
optimizing. This pipeline can often be configured by the user, either
at a high level to specify the overall degree of optimization desired,
or at a low level to enable or disable specific optimizations, however
for a given invocation of the compiler the pipeline is fixed. The
pass pipelines for common optimization levels are balanced to work
well with the widest range of code which may pass through the
compiler, however for any given piece of code it will not necessarily
be optimal. In many compilers the user may tune the pipeline for a
given program or source file, however this is a non-trivial manual
task which requires a lot of trial and error, and a lot of
iterations of compilation.

<-- Picture of typical approach taken when compiling -->

From the perspective of the user, it would be reassuring to know
that the compiler is not missing optimization opportunities. If the
compiler could be relied on to make more precise optimization decisions
it could also be possible to simplify the user experience by
not requiring them to manually disable and enable flags to
squeeze maximum performance from their code.

From the perspective of the compiler writer, it would be nice to
avoid having to manually tune the pass pipeline for each new
processor and CPU. For a given architecture, there can exist a vast
array of CPUs which vary in the hardware features supported and
the intricacies of their microarchitecture. (Example of ARM?)
Tuning the compiler for each of these variants is laborious. 

A further point of interest is that most compilers currently
optimize for speed, and give less attention to code size, energy,
or any other interesting metrics. In general optimization can only
be done with one goal (speed/code size) and it is not possible to
express more complex optimizations goals such as optimizing for
energy and code size at the same time, or optimizing for energy
after a code size target is achieved.

//As a brief test, we did a
//run of combined elimination optimizing for time and found we could
//achieve a ?% improvement compared to -O3, however when we did the
//same but optimized for energy, we found we could achieve a ?%
//improvement (do this analysis).

If selecting the best compiler configuration can be automated,
then there are several potential benefits which can be realized.
  
* Remove complexity from the compiler to tune optimization
  pipelines for different ISAs, microarchitectures, CPU features
  sets, and metrics.
* Provide the user a more straightforward interface, as well
  as better performance of resultant programs.
* Control optimizations at a finer granularity (file, module) to
  realize performance improvements which cannot be realized in current
  compilers (which can only control the pass pipeline at a
  per-invocation level).
* Optimize for new metrics such as energy or compilation speed.
* Provide a more powerful interface to control tradeoffs
  between different metrics, such as balancing energy and size.

<-- Diagram of more ideal optimization approaches (module and function level) -->

= Current optimization approach =

As it stands, most compilers appear to use a fixed pipeline of
passes when optimizing. However there has been other work to make
pass control more powerful. Some of these techniques exist in
contemporary compilers, however many only appear in research.

* Profile-guided optimization: This is common in many open source
  and propreitary compilers now. Optimization is tuned based on
  profiling data from running the program with representative
  data. The profile provides information about the hot-spots in
  the code, which can be used to inform the ordering of basic
  blocks and the ferocity of inlining.
* <-- MILEPOST -->
* <-- MAGEEC -->
* <-- Citations to other authors work -->

= Terminology =

* Program Unit - A contiguous part of a program which can be uniquely
  identifier in both the original source code and the final executable.
  Examples are modules, functions and possibly loops and basic blocks.
* Feature - A measurable and quantifiable aspect of a program unit, which
  represents an aspect of the program unit in some way. For example,
  the number of instructions in a basic block.
* Parameter - A value which can be controlled by some mechanism in the
  compiler to affect the decree or nature of optimization, with the
  effect of changing the object code produced by the program. For example:
  a flag indicating whether the 'dead code elimination' pass should be
  executed.
* Compiler configuration - A set of parameters used to compile an
  individual program unit. Multiple units of the same program may
  use the same configuration of the compiler.

= Theory for how MAGEEC works =

MAGEEC automates the selection of optimizations by using a
machine learner to may key decisions about the pass to
run and any tunable parameters which may be set. An
association is learned between features derived from the
input source code, and the appropriate parameters to use in
the compilation to build the executable. Like all machine
learning processes, there is a training stage; where the
machine learner is fed features and corresponding 'good'
parameters to build up an association; and a optimizing stage;
where the machine learner is used with a new, previously
unencountered program and used to select a set of parameters
which are likely to produce a good quality output executable.

<-- High level diagram of the process (with MAGEEC-instrumented compiler) -->

= High level features of MAGEEC =

MAGEEC using static features, which are derived ahead of time from the
input program. This has some advantages and disadvantages compared to
other approaches.

Advantages:
* Test programs do not have to be recompiled to receive a benefit. Interactive
  compilation techniques such as combined elimination require a large number
  of recompilation runs, and even just doing a single profiled run to
  derived dynamic features can be prohibitively difficult if the program is
  large or cannot easily be instrumented.
* Measure static features is simpler than measuring dynamic features. Dynamic
  features may not be precisely measureable, may depend on the environment,
  and require a more complex build stage.
* Measuring static features is quicker. The effect on compilation time can
  be minimized with static features, as they can be very easily derived from
  the source code or intermediate representation. This makes deployment
  far easier.
* In some cases it may not be possible to instrument and measure dynamic
  features. For example, an embedded system may not have enough memory
  for the overhead of instrumentation, or it may not be possible to
  measure some features on an embedded systems
  <-- Example (limitations in debug interface) -->

Disadvantages:
* Static features cannot express the effects of the input data on the
  program. Two program could have identical features, but exhibit very
  different performance characteristics depending on the data provided
  to it. Indeed, one program can also exhibit very different performance
  depending on the data provided to it. This can be mitigated, as the
  features of the program will often hint at the type of data used with
  it, however the problem cannot be mitigated.
* Static features are less representative of the input program. This
  relates to the previous point. Dynamic features are likely to be
  more representative of the input program than static features. This
  means that static features are likely to be far less effective than
  using dynamic features. It also means more training data is likely
  to be needed.
* If static features are derived too easier, they may be skewed by the
  structure of the input code. <-- Not really a problem -->

MAGEEC is open source under the GPL license. The aim is that by doing
this MAGEEC can be freely implemented in as wide range of compilers as
possible.

The design of MAGEEC makes it possible to use a variety of machine
learners through a common interface, and makes it possible to use
both command-line driven proprietary compilers, and open source
compilers. The interface to MAGEEC can either be a wrapper around
the compiler (ideal for a proprietary compiler), or can integrate
in the compiler itself, possibly through a plugin interface. Deeper
integration beneficial as it may allow compilation controlling
decision to be made at a finer granularity than program or module
level.

== Comparison with MAGEEC ==

The original MAGEEC was aimed at optimizing for energy on embedded
systems. The aim of this updated version was to also target high
performance application and also to improve the workflow to make
the optimization process more robust.

= MAGEEC components =

<-- High level diagram of gather+train with compiler, possibly later? (workflow?) -->

MAGEEC itself is not a machine learner, and is instead a
set of components and utilities which allow a compiler to
be intrumented with some 'machine-learning' capabilities. This
includes:

* An interface to derive static features from an input program
  (GCC feature extraction)
* A mechanism to drive the compiler to exercise various
  parameters during the gather phase. (GCC wrapper)
* A file format to record features, compilations, and results
  in a centralized place. (SQLite)
* An interface to a number of machine learners, and the
  machine learners themselves.
* Tools and scripts to access the file, and drive the training
  and optimization process.

<-- High level diagram of MAGEEC components -->
<-- Design of MAGEEC in general -->

We now describe each of these components in more detail, and
their current implementations.

== Feature extraction ==

=== Interface ===

Feature extraction involves taking an input source program, and
outputting a set of features for each of the individual
program units in the program. Each source file can produce
multiple sets of features for each different program unit in 
the file. So the module will produce a feature set, and each
function could produce its own feature set. It is even feasible
that feature could be generated for each basic block or loop.

MAGEEC provides an interface to create a set of features. How
exactly the features are extracted is up to the program doing the
feature extraction, and the features which are extracted is also
at the whim of the feature extractor. The limitations placed by
MAGEEC are that the type of the features must be one of a set of
known types (this set of types is common with the types which
the machine learners can handle), and a given extracted feature
is consistently identified by the name numerical identifier.

The interface to MAGEEC allows a feature extractor to insert a
new set of features into the file for a program unit, and
receive a unique identifier for those features back.

=== GCC plugin ===

As well as the interface to the feature extractor, a feature
extractor plugin in also provided for the GCC compiler. This
plugin can be provided as a plugin to gcc, and used to
extract module and function features from the program.

This plugin takes the input program and outputs a .csv file
identifing an individual program unit in the source code, associating
it with the unique identifier of the feature set for that
program unit.

The benefit of the GCC feature extractor is that it allows feature
extraction to be performed on C, C++ and Fortran code without
requiring a compiler-specific feature extractor to be written.
Provided code can be built with GCC, feature extraction can be
achieved. This makes it ideal for instrumenting propreitary
compilers.

The features extracted by the GCC feature extractor are as follows:

<-- List of features -->
<-- Description of features -->

<-- High level Diagram of feature extraction process -->
<-- Diagram of feature extraction plugin process -->

== Compiler driver ==

In order to drive the training and optimization process, there needs
to be a mechanism to control the parameters provided to the compiler.
When training, it needs to be possible to run the compiler under a
number of controlled configurations, and record each of these
compilations in the MAGEEC database for later use. During optimization
it needs to be possible to call out to MAGEEC to make decisions using
a trained machine learner and then use these decisions to control
compilation parameters.

=== Driver interface ==

MAGEEC offers an interface, which allows the compiler, or a wrapper
controlling the compiler to store a record of each program unit
being compiled, the features associated with that program unit, and
parameters describing the configuration of the compiler for that
compilation. The interface takes all of the information describing
the compilation of a program unit; the features and parameters
involved in the compilation, and returns a unique identifier for
that compilation. Later, results will be associated with the
compilation, and then there will be enough information to train
a machine learner.

For optimization, an interface is also offered which allows the driver
to use a previously trained machine learner to make decisions about
parameters. The driver can query a machine learner with a set of
feature, and receive a decision. It can then use this decision to control
the compilation of a program unit.

== GCC wrapper driver ==

MAGEEC offers one concrete interface to drive GCC for training and
optimization purposes which takes the form of a GCC wrapper. This
driver is a wrapper around the gcc/gfortran/g++ drivers, which
intecepts and controls the optimization flags passed to it.

When training, the driver tracks the optimization flags being used
with the driver and records a compilation in the database for each
program unit.

When optimizing, the driver strips optimization flags and instead
replaces the flags with flags derived through multiple calls out
to the MAGEEC interface to the machine learner. It uses previously
extracted features to inform this process.

Because the GCC wrapper driver works externally to the compiler, it
does not require any invasive changes to the compiler in order to
control the passes. It also supports multiple version of GCC easily
and can support Fortran and C++ code through the same driver.
Additionally because it uses the public documented interface to the
compiler, it is more stable as it cannot create broken pass pipelines <-- WUT? -->

A disadvantage of the GCC driver is that it is only able to make decisions on
a per-module level. If the driver instead operated internally to GCC it could
be possible to enable or disable passes on a per function basis. This
severely limits the optimization potential.

<-- Flags vs internal pass decisions -->
<-- High level diagram of compilation process for gather + optimize -->
<-- Diagram of GCC wrapper driver process -->

== MAGEEC File Format ==

Internally MAGEEC uses an SQLite database as an intermediate format.
This MAGEEC database is the main file format, and is used to record
the intermediate stages of training. It holds extracted features,
records of each compilation and the accompanying parameters and features
for it, results, trained machine learners, and various debug information.

When the feature extractor is run, features are added to the database,
and a handle to an entry in the database is returned. Likewise,
when a program unit is compiled with a with a set of parameters,
a compilation is recorded in the database an an identifier to it
is returned. This compilation can then have a result value associated
with it later.

There are a few reasons for using an SQLite database as an
intermediate format:

* The data by its nature contains relations which make it conducive
  to using a database. By using a database, this relations can be
  maintained and enforced.
* The database is a familiar conceptual model which is easier to
  understand.
* sqlitebrowser can be used to query the database manually, making
  it easier to debug and manually fix if it is ever necessary.
* SQLite allows multiple processes to read/write the same file,
  which is useful for parallelizing the training process for
  performance. Doing this with flat files would be more complex,
  and require reimplementing functionality already offered by
  SQLite.
* SQLite is quite high performance, and offers resilience against
  corrupt data out of the box.

The main disadvantage to using SQLite is that it adds some
complexity to the interfaces, and requires more boilerplate code
to interface than a simpler flat file solution would.

=== Database structure ===

Below is a description of the file format structure and the
associated relations.

<-- Database structure diagram -->

== Machine learner interface ==

The machine learner interface provides the capability to train
a machine learner given a set of compilations and their results,
as well as make decisions using a previously trained machine learner.

When training, MAGEEC provides the machine learner with all of the
results data for all of the compilations which have been performed.
This associates a set of inputs features compiled using a particular
set of parameters to the compiler, with a single output result
value. The interface also provides a set of parameters which the
user may wish to make decisions on, and which therefore forms the set
of parameters which the machine learner should train to make a
decision against.

The machine learner can use the dump of data however it likes.
However, the end result of the training process is a single binary
blob which is opaque to MAGEEC and the MAGEEC file format, but
can be deciphered by the machine learner at a later time when it
is required to make a decision. This binary blob is returned by
the machine learner and then stored by MAGEEC in the database for
use later.

When optimizing, the blob for the machine learner is retrieved
and an instance of the machine learner is instantiated. The
user of the machine learner can then query the machine learner
to make decisions about parameters which the machine learner
encountered when training. The machine learner is provided the
decision to make, as well as a set of features to use in order
to make the decision. The machine learner can choose to make a
decision, or it can refuse, leaving it up to the user to then
make the decision itself.

The features provided to the machine learner are made up of a
unique identifier, as well as a 'type'. The unique identifier
is guaranteed by the feature extractor to unique identify the
same feature every time, and the type is one of a number of
types specified by MAGEEC.

The parameters which are provided to the machine learner also
have a unique identifier which is guaranteed to identify the
same parameter in the compiler which is being used. The
types of the parameters is also an identifier for a type specified
by MAGEEC.

<-- Feature types, parameter types, decisions, etc? -->

Currently, there is no ability to do incremental learning.

<-- No ability to do incremental learning -->
<-- Diagram of machine learner inputs and outputs for train + optimize -->

The machine learning interface is used by the standalone tool
in order to train a database based on generated results, and
it is used by the GCC wrapper driver in order to make decisions
during compilation.

In MAGEEC there currently exist two machine learners, a machine learner
based on the C5.0 classifier, and a machine learner based on a one
nearest-neighbor search. These are built as libraries which the
GCC wrapper then links against.

A user can create a new machine learner, provided it implements the
machine learner interface. It is then possible to use this machine
learner as a plugin to the standalone tool and GCC wrapper, by
building the machine learner as a dynamic library which the
tools can link against.

=== C5.0 Machine learner ===

C5.0 is a machine learning which creates a classifier tree based on
the input features. For MAGEEC, a C5.0 classifier tree is created
for each decision which may be made (corresponding to a parameter
in the compilation). The machine learner is provided the best
result for each distinct set of features, and it uses these
to build a classification tree for each parameter. The classification
tree for each of the parameters are concatenated together and
stored in the blob.

When a decision must be made, the appropriate classifier tree is
found, and the features for the program unit are used to determine
the classification for that decision. For a boolean decision, the
resulting decision could be 'true', 'false', or 'native' if the
machine learner is not able to make a decision.

<-- Diagram of classifier trees for MAGEEC -->
<-- Diagram of use of a classifier tree for MAGEEC -->

Advantages of C5.0 are that the interface is straightforward
and the decision trees are human readable.

=== 1-NN Machine learner ===

One nearest neighbor is a conceptually very simple classification
method. Each feature set form a point in n-dimensional space, where
each feature is mapped to an individual axis. Each feature set 'point'
is associated with a parameter set, which is the parameter set for
the best result for that feature set in the results data. When a
decision needs to be made, the closest feature set to the input
set is found in the training data, and the decision is made based
on the parameters in that closest point.

1-NN is a very simply classification technique, and is very easy
to understand. 

<-- Diagram of 1-NN classification for MAGEEC -->

== Toolings ==

As well as the key MAGEEC interfaces and components, there are also
a few tasks which are achieved through tooling and python scripts.

=== Standalone MAGEEC tool ===

This tool exists to do a few tasks which can't be done by either
the feature extractor or compiler driver. This includes; adding
results into the database, training a database using a machine
learner; and creating and merging databases files. It is a simple
command line tool.

=== Feature extraction script ===

This script exists to build and feature extract a program using the
GCC feature extractor plugin. It simplifies the interface to the
plugin, and automatically handles autoconf and CMake build systems.

=== Combined elimination script ===

This script performs the combined elimination process as described in
<-- Citation -->. This script exists in order to do an iterative
search towards the best combination of compilation flags for a
benchmark, which can be used to effectively train many machine learners.
The naive way to find good flag combinations is to do a random
search, however combined elimination finds more optimal results in
far fewer runs. Even compared to doing a fractional factorial design
combined elimination is much more effective.

The combined elimination process starts with all flags enabled,
and then attempts to disable flags in groups in order to find a
more optimal set of flags for a given program. It has been shown
to be more effective than other iteractive compilation techniques.

<-- Combined elimination diagram -->

Combined elimination is used by MAGEEC primarily for finding good
data points for the training process.

== The MAGEEC library ==

The various interfaces exposed by MAGEEC, and used by the GCC
feature extractor plugin, GCC wrapper driver and standalone tool
are all implemented in a core library which the MAGEEC tools,
and other software which wishes to use with MAGEEC can link
against. By including this in a library, it means that integration
with MAGEEC can be embedded into other compilers. This could
be used to perform feature extraction from within another
compiler, or to allow MAGEEC to alter the internal pass
sequences in a compiler.

<-- Workflow -->

* Gather
* Training

<-- Experimentation -->

* Features
* Machine learners
* Optimizing for size
* Optimizing for energy
* NEALE






Unlike other work in this area, the extracted features are
static features derived from the input source code, rather
than dynamic features derived from an execution of a
program. This has the advantage that features can 

are made based on static features 

MAGEEC automates the selection of optimizations by using a
machine learner to make key decisions about the optimizations
to run, based on a number of statically derived features of
the program. It is designed to either integrate with the
compiler, or be included in a wrapper around the compiler
driver.

WORKFLOW

= Comparison with existing techniques = 

* PGO
* Static vs dynamic features
* Production ready (TM)

= MAGEEC features =

* Cross-platform (in theory)
* Flexible to different compilers (GCC implemented)
* Flexible to different languages (C, C++, Fortran implemented)
* Integrated or standalone (closed source compilers)
* Library design
* Based on flags or compiler internal decisions
* Open source
* Plugin interface for different machine learners

= MAGEEC design =

There are a few components to the whole system:

HIGH LEVEL COMPONENTS

* A feature extractor
  This takes the input source file and extracts interesting
  features which quantity each of the program units which
  comprise the file. The feature extractor can be integrated
  in the compiler, or part of a standalone tool.
* A compiler stub or wrapper
  When gathering, this is used to record the parameters
  which make up the compiler configuation
  When optimizing, this uses extracted features to
  query the machine learner to make decisions about
  the value which compilation parameters should take.

MAGEEC DESIGN

MAGEEC is based very strongly around its file format which is
an SQLite database. The database holds intermediate data during
gathering 

MAGEEC COMPONENTS

FEATURES




MAGEEC is based around a 
MAGEEC itself is made up of a few components, which are as
follows:

* The core MAGEEC library
  This library provides an interface to the majority of
  the functionality. It is used by the feature extractor
  MAGEEC database. 
* A standalone feature extractor


* Justifications for decisions
* Important terms
* Combined elimination
* Experiments
* Results
* Limitations
* Future work
* Cross platform
* Multi-compiler
* Based on flags or other
* Multi-language (gfortran, gcc, g++)
* NEALE/Hartree systems
* Modules/Libraries
* Gathering
* Adding results + training
* Workflow
* Optimizing
* Benchmarks (mantevo)
* Feature quality
* Diverse feature sets
* Tools (gcc driver, standalone tool, feature extractor, scripts)
  * Documentation of tools and their flags
* Considerations for benchmarking on supercomputers
* Terms (Features/Parameters/Compilation/Program unit)
* SQLite as an intermediate format
  * Minimize intermediate files
  * Structured, easy to debug (sqlitebrowser, sqlite3)
  * Simple conceptual model
  * Multiprocess access and good performance
* Allinea MAP
* Machine learners
  * C5.0
  * 1NN
* Design decisions
* Open source
